
==> Audit <==
|------------|--------------------------------|----------|-----------------|---------|---------------------|---------------------|
|  Command   |              Args              | Profile  |      User       | Version |     Start Time      |      End Time       |
|------------|--------------------------------|----------|-----------------|---------|---------------------|---------------------|
| start      |                                | minikube | HARISH-SEN\user | v1.36.0 | 09 Sep 25 11:50 IST | 09 Sep 25 11:56 IST |
| start      |                                | minikube | HARISH-SEN\user | v1.36.0 | 09 Sep 25 16:56 IST |                     |
| start      |                                | minikube | HARISH-SEN\user | v1.36.0 | 09 Sep 25 18:11 IST | 09 Sep 25 18:12 IST |
| docker-env |                                | minikube | HARISH-SEN\user | v1.36.0 | 09 Sep 25 18:13 IST | 09 Sep 25 18:13 IST |
| docker-env | minikube docker-env --shell    | minikube | HARISH-SEN\user | v1.36.0 | 09 Sep 25 18:19 IST | 09 Sep 25 18:19 IST |
|            | cmd                            |          |                 |         |                     |                     |
| ip         |                                | minikube | HARISH-SEN\user | v1.36.0 | 09 Sep 25 20:59 IST | 09 Sep 25 20:59 IST |
| start      |                                | minikube | HARISH-SEN\user | v1.36.0 | 09 Sep 25 22:41 IST | 09 Sep 25 22:42 IST |
| ip         |                                | minikube | HARISH-SEN\user | v1.36.0 | 09 Sep 25 23:05 IST | 09 Sep 25 23:05 IST |
| start      |                                | minikube | HARISH-SEN\user | v1.36.0 | 10 Sep 25 19:56 IST | 10 Sep 25 20:01 IST |
| docker-env |                                | minikube | HARISH-SEN\user | v1.36.0 | 10 Sep 25 20:04 IST | 10 Sep 25 20:04 IST |
| docker-env |                                | minikube | HARISH-SEN\user | v1.36.0 | 10 Sep 25 20:05 IST | 10 Sep 25 20:05 IST |
| ip         |                                | minikube | HARISH-SEN\user | v1.36.0 | 10 Sep 25 20:40 IST | 10 Sep 25 20:40 IST |
| dashboard  |                                | minikube | HARISH-SEN\user | v1.36.0 | 10 Sep 25 20:47 IST |                     |
| service    | spring-boot-k8s                | minikube | HARISH-SEN\user | v1.36.0 | 10 Sep 25 21:14 IST | 10 Sep 25 21:20 IST |
| service    | spring-boot-k8s                | minikube | HARISH-SEN\user | v1.36.0 | 10 Sep 25 21:26 IST | 10 Sep 25 21:26 IST |
| service    | spring-boot-k8s                | minikube | HARISH-SEN\user | v1.36.0 | 10 Sep 25 21:27 IST |                     |
| tunnel     |                                | minikube | HARISH-SEN\user | v1.36.0 | 10 Sep 25 21:31 IST |                     |
| service    | springboot-single-endpoint-eks | minikube | HARISH-SEN\user | v1.36.0 | 11 Sep 25 11:16 IST |                     |
| start      |                                | minikube | HARISH-SEN\user | v1.36.0 | 11 Sep 25 11:16 IST | 11 Sep 25 11:17 IST |
| service    | springboot-single-endpoint-eks | minikube | HARISH-SEN\user | v1.36.0 | 11 Sep 25 11:17 IST |                     |
| service    | springboot-single-endpoint-eks | minikube | HARISH-SEN\user | v1.36.0 | 11 Sep 25 11:17 IST |                     |
| service    | springboot-single-endpoint-eks | minikube | HARISH-SEN\user | v1.36.0 | 11 Sep 25 11:25 IST |                     |
| service    | springboot-single-endpoint-eks | minikube | HARISH-SEN\user | v1.36.0 | 11 Sep 25 11:43 IST |                     |
| dashboard  |                                | minikube | HARISH-SEN\user | v1.36.0 | 11 Sep 25 11:44 IST |                     |
| service    | kubectl describe svc           | minikube | HARISH-SEN\user | v1.36.0 | 11 Sep 25 12:38 IST |                     |
|            | springboot-single-endpoint-eks |          |                 |         |                     |                     |
| service    | kubectl describe svc           | minikube | HARISH-SEN\user | v1.36.0 | 11 Sep 25 12:40 IST |                     |
|            | springboot-single-endpoint-eks |          |                 |         |                     |                     |
| delete     |                                | minikube | HARISH-SEN\user | v1.36.0 | 11 Sep 25 12:41 IST | 11 Sep 25 12:41 IST |
| start      |                                | minikube | HARISH-SEN\user | v1.36.0 | 11 Sep 25 12:41 IST | 11 Sep 25 12:43 IST |
| service    | kubectl describe svc           | minikube | HARISH-SEN\user | v1.36.0 | 11 Sep 25 21:09 IST |                     |
|            | springboot-app-ek8             |          |                 |         |                     |                     |
|------------|--------------------------------|----------|-----------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/09/11 12:41:28
Running on machine: HARISH-SEN
Binary: Built with gc go1.24.0 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0911 12:41:28.318796   18944 out.go:345] Setting OutFile to fd 496 ...
I0911 12:41:28.319358   18944 out.go:392] TERM=xterm,COLORTERM=, which probably does not support color
I0911 12:41:28.319358   18944 out.go:358] Setting ErrFile to fd 496...
I0911 12:41:28.319358   18944 out.go:392] TERM=xterm,COLORTERM=, which probably does not support color
I0911 12:41:28.340633   18944 out.go:352] Setting JSON to false
I0911 12:41:28.345377   18944 start.go:130] hostinfo: {"hostname":"HARISH-SEN","uptime":43455,"bootTime":1757531233,"procs":293,"os":"windows","platform":"Microsoft Windows 11 Pro","platformFamily":"Standalone Workstation","platformVersion":"10.0.26100.6584 Build 26100.6584","kernelVersion":"10.0.26100.6584 Build 26100.6584","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"aba62192-8d45-4b13-a8c0-1a4496a68209"}
W0911 12:41:28.346055   18944 start.go:138] gopshost.Virtualization returned error: not implemented yet
I0911 12:41:28.348363   18944 out.go:177] * minikube v1.36.0 on Microsoft Windows 11 Pro 10.0.26100.6584 Build 26100.6584
I0911 12:41:28.351776   18944 notify.go:220] Checking for updates...
I0911 12:41:28.354639   18944 driver.go:404] Setting default libvirt URI to qemu:///system
I0911 12:41:28.355320   18944 global.go:112] Querying for installed drivers using PATH=C:\Users\user\bin;C:\Program Files\Git\mingw64\bin;C:\Program Files\Git\usr\local\bin;C:\Program Files\Git\usr\bin;C:\Program Files\Git\usr\bin;C:\Program Files\Git\mingw64\bin;C:\Program Files\Git\usr\bin;C:\Users\user\bin;C:\Program Files\Common Files\Oracle\Java\javapath;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0;C:\Windows\System32\OpenSSH;C:\Program Files\Docker\Docker\resources\bin;C:\Program Files\Kubernetes\Minikube;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0;C:\WINDOWS\System32\OpenSSH;C:\Program Files\Git\cmd;C:\Program Files\nodejs;C:\Program Files\PuTTY;C:\Users\user\AppData\Local\Microsoft\WindowsApps;C:\Program Files\JetBrains\IntelliJ IDEA 2025.2.1\bin;C:\Users\user\AppData\Local\Programs\Microsoft VS Code\bin;C:\Users\user\AppData\Roaming\npm;C:\Program Files\Git\usr\bin\vendor_perl;C:\Program Files\Git\usr\bin\core_perl
I0911 12:41:28.363894   18944 global.go:133] podman default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "podman": executable file not found in %PATH% Reason: Fix:Install Podman Doc:https://minikube.sigs.k8s.io/docs/drivers/podman/ Version:}
I0911 12:41:28.363894   18944 global.go:133] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0911 12:41:33.021123   18944 global.go:133] hyperv default: true priority: 8, state: {Installed:true Healthy:false Running:false NeedsImprovement:false Error:Hyper-V requires Administrator privileges Reason: Fix:Right-click the PowerShell icon and select Run as Administrator to open PowerShell in elevated mode. Doc: Version:}
I0911 12:41:33.030917   18944 global.go:133] qemu2 default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "qemu-system-x86_64": executable file not found in %PATH% Reason: Fix:Install qemu-system Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/qemu/ Version:}
I0911 12:41:33.049529   18944 global.go:133] virtualbox default: true priority: 6, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:unable to find VBoxManage in $PATH Reason: Fix:Install VirtualBox Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/virtualbox/ Version:}
I0911 12:41:33.056079   18944 global.go:133] vmware default: false priority: 5, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "vmrun": executable file not found in %PATH% Reason: Fix:Install vmrun Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}
I0911 12:41:33.153872   18944 docker.go:123] docker version: linux-28.3.3:Docker Desktop 4.45.0 (203075)
I0911 12:41:33.158714   18944 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0911 12:41:33.951511   18944 info.go:266] docker info: {ID:2d497caa-21a7-4c79-af70-5ba80eeca4ed Containers:3 ContainersRunning:3 ContainersPaused:0 ContainersStopped:0 Images:10 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:94 OomKillDisable:false NGoroutines:135 SystemTime:2025-09-11 07:11:33.854525938 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:14 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:6 MemTotal:3943407616 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.3.3 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.9.11] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.27.0-desktop.1] map[Name:cloud Path:C:\Program Files\Docker\cli-plugins\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.4.21] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.39.2-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.42] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.2.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.31] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:C:\Program Files\Docker\cli-plugins\docker-mcp.exe SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:v0.15.0] map[Name:model Path:C:\Program Files\Docker\cli-plugins\docker-model.exe SchemaVersion:0.1.0 ShortDescription:Docker Model Runner (EXPERIMENTAL) Vendor:Docker Inc. Version:v0.1.39] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.18.3]] Warnings:<nil>}}
I0911 12:41:33.952042   18944 global.go:133] docker default: true priority: 9, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0911 12:41:33.952042   18944 driver.go:326] not recommending "ssh" due to default: false
I0911 12:41:33.952042   18944 driver.go:321] not recommending "hyperv" due to health: Hyper-V requires Administrator privileges
I0911 12:41:33.952042   18944 driver.go:361] Picked: docker
I0911 12:41:33.952042   18944 driver.go:362] Alternatives: [ssh]
I0911 12:41:33.952042   18944 driver.go:363] Rejects: [podman hyperv qemu2 virtualbox vmware]
I0911 12:41:33.953091   18944 out.go:177] * Automatically selected the docker driver
I0911 12:41:33.954142   18944 start.go:304] selected driver: docker
I0911 12:41:33.954142   18944 start.go:908] validating driver "docker" against <nil>
I0911 12:41:33.954142   18944 start.go:919] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0911 12:41:33.963770   18944 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0911 12:41:34.370301   18944 info.go:266] docker info: {ID:2d497caa-21a7-4c79-af70-5ba80eeca4ed Containers:3 ContainersRunning:3 ContainersPaused:0 ContainersStopped:0 Images:10 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:94 OomKillDisable:false NGoroutines:135 SystemTime:2025-09-11 07:11:34.346261486 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:14 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:6 MemTotal:3943407616 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.3.3 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.9.11] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.27.0-desktop.1] map[Name:cloud Path:C:\Program Files\Docker\cli-plugins\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.4.21] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.39.2-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.42] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.2.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.31] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:C:\Program Files\Docker\cli-plugins\docker-mcp.exe SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:v0.15.0] map[Name:model Path:C:\Program Files\Docker\cli-plugins\docker-model.exe SchemaVersion:0.1.0 ShortDescription:Docker Model Runner (EXPERIMENTAL) Vendor:Docker Inc. Version:v0.1.39] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.18.3]] Warnings:<nil>}}
I0911 12:41:34.370831   18944 start_flags.go:311] no existing cluster config was found, will generate one from the flags 
I0911 12:41:34.485496   18944 start_flags.go:394] Using suggested 2200MB memory alloc based on sys=7876MB, container=3760MB
I0911 12:41:34.486595   18944 start_flags.go:958] Wait components to verify : map[apiserver:true system_pods:true]
I0911 12:41:34.487187   18944 out.go:177] * Using Docker Desktop driver with root privileges
I0911 12:41:34.490253   18944 cni.go:84] Creating CNI manager for ""
I0911 12:41:34.490253   18944 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0911 12:41:34.490253   18944 start_flags.go:320] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I0911 12:41:34.490767   18944 start.go:347] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\user:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0911 12:41:34.491856   18944 out.go:177] * Starting "minikube" primary control-plane node in "minikube" cluster
I0911 12:41:34.493488   18944 cache.go:121] Beginning downloading kic base image for docker with docker
I0911 12:41:34.494002   18944 out.go:177] * Pulling base image v0.0.47 ...
I0911 12:41:34.496055   18944 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0911 12:41:34.496055   18944 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local docker daemon
I0911 12:41:34.498107   18944 preload.go:146] Found local preload: C:\Users\user\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4
I0911 12:41:34.498107   18944 cache.go:56] Caching tarball of preloaded images
I0911 12:41:34.498744   18944 preload.go:172] Found C:\Users\user\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0911 12:41:34.499312   18944 cache.go:59] Finished verifying existence of preloaded tar for v1.33.1 on docker
I0911 12:41:34.500473   18944 profile.go:143] Saving config to C:\Users\user\.minikube\profiles\minikube\config.json ...
I0911 12:41:34.501031   18944 lock.go:35] WriteFile acquiring C:\Users\user\.minikube\profiles\minikube\config.json: {Name:mk9ad62bba176375f4633db264b56909ce29fd55 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0911 12:41:34.749999   18944 cache.go:150] Downloading gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b to local cache
I0911 12:41:34.751604   18944 localpath.go:146] windows sanitize: C:\Users\user\.minikube\cache\kic\amd64\kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar -> C:\Users\user\.minikube\cache\kic\amd64\kicbase_v0.0.47@sha256_6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar
I0911 12:41:34.752125   18944 localpath.go:146] windows sanitize: C:\Users\user\.minikube\cache\kic\amd64\kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar -> C:\Users\user\.minikube\cache\kic\amd64\kicbase_v0.0.47@sha256_6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar
I0911 12:41:34.752648   18944 image.go:65] Checking for gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local cache directory
I0911 12:41:34.754762   18944 image.go:68] Found gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local cache directory, skipping pull
I0911 12:41:34.754762   18944 image.go:137] gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b exists in cache, skipping pull
I0911 12:41:34.755280   18944 cache.go:153] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b as a tarball
I0911 12:41:34.755280   18944 cache.go:163] Loading gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b from local cache
I0911 12:41:34.755280   18944 localpath.go:146] windows sanitize: C:\Users\user\.minikube\cache\kic\amd64\kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar -> C:\Users\user\.minikube\cache\kic\amd64\kicbase_v0.0.47@sha256_6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar
I0911 12:42:12.086736   18944 cache.go:165] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b from cached tarball
I0911 12:42:12.087278   18944 cache.go:230] Successfully downloaded all kic artifacts
I0911 12:42:12.089966   18944 start.go:360] acquireMachinesLock for minikube: {Name:mkb3e405ffb35ff57f19f1090207beea089db946 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0911 12:42:12.090502   18944 start.go:364] duration metric: took 536.1Âµs to acquireMachinesLock for "minikube"
I0911 12:42:12.091561   18944 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\user:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}
I0911 12:42:12.091561   18944 start.go:125] createHost starting for "" (driver="docker")
I0911 12:42:12.093688   18944 out.go:235] * Creating docker container (CPUs=2, Memory=2200MB) ...
I0911 12:42:12.097044   18944 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0911 12:42:12.097620   18944 client.go:168] LocalClient.Create starting
I0911 12:42:12.099849   18944 main.go:141] libmachine: Reading certificate data from C:\Users\user\.minikube\certs\ca.pem
I0911 12:42:12.100914   18944 main.go:141] libmachine: Decoding PEM data...
I0911 12:42:12.100914   18944 main.go:141] libmachine: Parsing certificate...
I0911 12:42:12.103112   18944 main.go:141] libmachine: Reading certificate data from C:\Users\user\.minikube\certs\cert.pem
I0911 12:42:12.103718   18944 main.go:141] libmachine: Decoding PEM data...
I0911 12:42:12.104294   18944 main.go:141] libmachine: Parsing certificate...
I0911 12:42:12.120476   18944 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0911 12:42:12.219762   18944 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0911 12:42:12.228976   18944 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I0911 12:42:12.228976   18944 cli_runner.go:164] Run: docker network inspect minikube
W0911 12:42:12.300960   18944 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0911 12:42:12.300960   18944 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0911 12:42:12.300960   18944 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0911 12:42:12.310269   18944 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0911 12:42:12.424842   18944 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc001fcecc0}
I0911 12:42:12.425402   18944 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I0911 12:42:12.433414   18944 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0911 12:42:12.610041   18944 network_create.go:108] docker network minikube 192.168.49.0/24 created
I0911 12:42:12.612511   18944 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I0911 12:42:12.631489   18944 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0911 12:42:12.767364   18944 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0911 12:42:12.852677   18944 oci.go:103] Successfully created a docker volume minikube
I0911 12:42:12.862021   18944 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b -d /var/lib
I0911 12:42:14.834659   18944 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b -d /var/lib: (1.9726386s)
I0911 12:42:14.834659   18944 oci.go:107] Successfully prepared a docker volume minikube
I0911 12:42:14.834659   18944 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0911 12:42:14.834659   18944 kic.go:194] Starting extracting preloaded images to volume ...
I0911 12:42:14.863123   18944 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\user\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b -I lz4 -xf /preloaded.tar -C /extractDir
I0911 12:42:40.589196   18944 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\user\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b -I lz4 -xf /preloaded.tar -C /extractDir: (25.7260728s)
I0911 12:42:40.590575   18944 kic.go:203] duration metric: took 25.7559155s to extract preloaded images to volume ...
I0911 12:42:40.606509   18944 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0911 12:42:41.563051   18944 info.go:266] docker info: {ID:2d497caa-21a7-4c79-af70-5ba80eeca4ed Containers:3 ContainersRunning:3 ContainersPaused:0 ContainersStopped:0 Images:11 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:93 OomKillDisable:false NGoroutines:131 SystemTime:2025-09-11 07:12:41.47528083 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:13 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:6 MemTotal:3943407616 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.3.3 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.9.11] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.27.0-desktop.1] map[Name:cloud Path:C:\Program Files\Docker\cli-plugins\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.4.21] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.39.2-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.42] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.2.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.31] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:C:\Program Files\Docker\cli-plugins\docker-mcp.exe SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:v0.15.0] map[Name:model Path:C:\Program Files\Docker\cli-plugins\docker-model.exe SchemaVersion:0.1.0 ShortDescription:Docker Model Runner (EXPERIMENTAL) Vendor:Docker Inc. Version:v0.1.39] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.18.3]] Warnings:<nil>}}
I0911 12:42:41.583849   18944 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0911 12:42:42.614835   18944 cli_runner.go:217] Completed: docker info --format "'{{json .SecurityOptions}}'": (1.0309865s)
I0911 12:42:42.625658   18944 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=2200mb --memory-swap=2200mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b
I0911 12:42:43.618425   18944 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0911 12:42:43.709613   18944 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0911 12:42:43.811712   18944 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0911 12:42:44.090422   18944 oci.go:144] the created container "minikube" has a running status.
I0911 12:42:44.090958   18944 kic.go:225] Creating ssh key for kic: C:\Users\user\.minikube\machines\minikube\id_rsa...
I0911 12:42:44.179734   18944 kic_runner.go:191] docker (temp): C:\Users\user\.minikube\machines\minikube\id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0911 12:42:44.617954   18944 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0911 12:42:44.726196   18944 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0911 12:42:44.726196   18944 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0911 12:42:45.308474   18944 kic.go:265] ensuring only current user has permissions to key file located at : C:\Users\user\.minikube\machines\minikube\id_rsa...
I0911 12:42:46.499045   18944 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0911 12:42:46.599829   18944 machine.go:93] provisionDockerMachine start ...
I0911 12:42:46.606219   18944 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0911 12:42:46.710410   18944 main.go:141] libmachine: Using SSH client type: native
I0911 12:42:46.750761   18944 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x4aa9e0] 0x4ad520 <nil>  [] 0s} 127.0.0.1 60002 <nil> <nil>}
I0911 12:42:46.750761   18944 main.go:141] libmachine: About to run SSH command:
hostname
I0911 12:42:47.348555   18944 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0911 12:42:47.349757   18944 ubuntu.go:169] provisioning hostname "minikube"
I0911 12:42:47.361368   18944 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0911 12:42:47.478169   18944 main.go:141] libmachine: Using SSH client type: native
I0911 12:42:47.479284   18944 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x4aa9e0] 0x4ad520 <nil>  [] 0s} 127.0.0.1 60002 <nil> <nil>}
I0911 12:42:47.479284   18944 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0911 12:42:47.968935   18944 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0911 12:42:47.977857   18944 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0911 12:42:48.062531   18944 main.go:141] libmachine: Using SSH client type: native
I0911 12:42:48.063070   18944 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x4aa9e0] 0x4ad520 <nil>  [] 0s} 127.0.0.1 60002 <nil> <nil>}
I0911 12:42:48.063070   18944 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0911 12:42:48.254403   18944 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0911 12:42:48.254403   18944 ubuntu.go:175] set auth options {CertDir:C:\Users\user\.minikube CaCertPath:C:\Users\user\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\user\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\user\.minikube\machines\server.pem ServerKeyPath:C:\Users\user\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\user\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\user\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\user\.minikube}
I0911 12:42:48.254934   18944 ubuntu.go:177] setting up certificates
I0911 12:42:48.256036   18944 provision.go:84] configureAuth start
I0911 12:42:48.263979   18944 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0911 12:42:48.337696   18944 provision.go:143] copyHostCerts
I0911 12:42:48.339284   18944 exec_runner.go:144] found C:\Users\user\.minikube/ca.pem, removing ...
I0911 12:42:48.339817   18944 exec_runner.go:203] rm: C:\Users\user\.minikube\ca.pem
I0911 12:42:48.340356   18944 exec_runner.go:151] cp: C:\Users\user\.minikube\certs\ca.pem --> C:\Users\user\.minikube/ca.pem (1070 bytes)
I0911 12:42:48.342535   18944 exec_runner.go:144] found C:\Users\user\.minikube/cert.pem, removing ...
I0911 12:42:48.342535   18944 exec_runner.go:203] rm: C:\Users\user\.minikube\cert.pem
I0911 12:42:48.343144   18944 exec_runner.go:151] cp: C:\Users\user\.minikube\certs\cert.pem --> C:\Users\user\.minikube/cert.pem (1115 bytes)
I0911 12:42:48.345293   18944 exec_runner.go:144] found C:\Users\user\.minikube/key.pem, removing ...
I0911 12:42:48.345293   18944 exec_runner.go:203] rm: C:\Users\user\.minikube\key.pem
I0911 12:42:48.345870   18944 exec_runner.go:151] cp: C:\Users\user\.minikube\certs\key.pem --> C:\Users\user\.minikube/key.pem (1675 bytes)
I0911 12:42:48.347477   18944 provision.go:117] generating server cert: C:\Users\user\.minikube\machines\server.pem ca-key=C:\Users\user\.minikube\certs\ca.pem private-key=C:\Users\user\.minikube\certs\ca-key.pem org=user.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0911 12:42:48.763951   18944 provision.go:177] copyRemoteCerts
I0911 12:42:48.775669   18944 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0911 12:42:48.783689   18944 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0911 12:42:48.894307   18944 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:60002 SSHKeyPath:C:\Users\user\.minikube\machines\minikube\id_rsa Username:docker}
I0911 12:42:49.153610   18944 ssh_runner.go:362] scp C:\Users\user\.minikube\machines\server.pem --> /etc/docker/server.pem (1176 bytes)
I0911 12:42:49.288561   18944 ssh_runner.go:362] scp C:\Users\user\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0911 12:42:49.381825   18944 ssh_runner.go:362] scp C:\Users\user\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1070 bytes)
I0911 12:42:49.424250   18944 provision.go:87] duration metric: took 1.1675063s to configureAuth
I0911 12:42:49.424250   18944 ubuntu.go:193] setting minikube options for container-runtime
I0911 12:42:49.430199   18944 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0911 12:42:49.437343   18944 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0911 12:42:49.520400   18944 main.go:141] libmachine: Using SSH client type: native
I0911 12:42:49.520931   18944 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x4aa9e0] 0x4ad520 <nil>  [] 0s} 127.0.0.1 60002 <nil> <nil>}
I0911 12:42:49.520931   18944 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0911 12:42:49.698054   18944 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0911 12:42:49.698054   18944 ubuntu.go:71] root file system type: overlay
I0911 12:42:49.702278   18944 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0911 12:42:49.711048   18944 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0911 12:42:49.782608   18944 main.go:141] libmachine: Using SSH client type: native
I0911 12:42:49.783664   18944 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x4aa9e0] 0x4ad520 <nil>  [] 0s} 127.0.0.1 60002 <nil> <nil>}
I0911 12:42:49.783664   18944 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0911 12:42:50.040679   18944 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0911 12:42:50.053698   18944 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0911 12:42:50.223751   18944 main.go:141] libmachine: Using SSH client type: native
I0911 12:42:50.224884   18944 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x4aa9e0] 0x4ad520 <nil>  [] 0s} 127.0.0.1 60002 <nil> <nil>}
I0911 12:42:50.224884   18944 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0911 12:42:53.267316   18944 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2025-04-18 09:50:48.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2025-09-11 07:12:50.032524316 +0000
@@ -1,46 +1,49 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0911 12:42:53.269612   18944 machine.go:96] duration metric: took 6.6692455s to provisionDockerMachine
I0911 12:42:53.270138   18944 client.go:171] duration metric: took 41.1719921s to LocalClient.Create
I0911 12:42:53.270138   18944 start.go:167] duration metric: took 41.1730933s to libmachine.API.Create "minikube"
I0911 12:42:53.271217   18944 start.go:293] postStartSetup for "minikube" (driver="docker")
I0911 12:42:53.271953   18944 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0911 12:42:53.276289   18944 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0911 12:42:53.280261   18944 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0911 12:42:53.343287   18944 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:60002 SSHKeyPath:C:\Users\user\.minikube\machines\minikube\id_rsa Username:docker}
I0911 12:42:53.499658   18944 ssh_runner.go:195] Run: cat /etc/os-release
I0911 12:42:53.509332   18944 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0911 12:42:53.509332   18944 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0911 12:42:53.509332   18944 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0911 12:42:53.509332   18944 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0911 12:42:53.509863   18944 filesync.go:126] Scanning C:\Users\user\.minikube\addons for local assets ...
I0911 12:42:53.512500   18944 filesync.go:126] Scanning C:\Users\user\.minikube\files for local assets ...
I0911 12:42:53.513142   18944 start.go:296] duration metric: took 241.1897ms for postStartSetup
I0911 12:42:53.521509   18944 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0911 12:42:53.600761   18944 profile.go:143] Saving config to C:\Users\user\.minikube\profiles\minikube\config.json ...
I0911 12:42:53.608012   18944 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0911 12:42:53.611704   18944 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0911 12:42:53.676947   18944 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:60002 SSHKeyPath:C:\Users\user\.minikube\machines\minikube\id_rsa Username:docker}
I0911 12:42:53.796016   18944 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0911 12:42:53.810419   18944 start.go:128] duration metric: took 41.7171243s to createHost
I0911 12:42:53.810951   18944 start.go:83] releasing machines lock for "minikube", held for 41.7204487s
I0911 12:42:53.819914   18944 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0911 12:42:53.906676   18944 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I0911 12:42:53.910161   18944 ssh_runner.go:195] Run: cat /version.json
I0911 12:42:53.919718   18944 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0911 12:42:53.920894   18944 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0911 12:42:53.990021   18944 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:60002 SSHKeyPath:C:\Users\user\.minikube\machines\minikube\id_rsa Username:docker}
I0911 12:42:53.991299   18944 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:60002 SSHKeyPath:C:\Users\user\.minikube\machines\minikube\id_rsa Username:docker}
W0911 12:42:54.106765   18944 start.go:867] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I0911 12:42:54.121445   18944 ssh_runner.go:195] Run: systemctl --version
I0911 12:42:54.135870   18944 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0911 12:42:54.149351   18944 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0911 12:42:54.174385   18944 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0911 12:42:54.182564   18944 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0911 12:42:54.243232   18944 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist, /etc/cni/net.d/100-crio-bridge.conf] bridge cni config(s)
I0911 12:42:54.243794   18944 start.go:495] detecting cgroup driver to use...
I0911 12:42:54.243794   18944 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0911 12:42:54.248720   18944 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0911 12:42:54.279193   18944 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0911 12:42:54.303066   18944 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0911 12:42:54.324104   18944 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0911 12:42:54.326283   18944 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0911 12:42:54.341202   18944 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0911 12:42:54.370435   18944 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0911 12:42:54.391494   18944 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0911 12:42:54.412119   18944 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0911 12:42:54.430097   18944 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0911 12:42:54.450596   18944 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0911 12:42:54.470750   18944 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0911 12:42:54.505213   18944 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0911 12:42:54.531667   18944 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0911 12:42:54.551406   18944 ssh_runner.go:195] Run: sudo systemctl daemon-reload
W0911 12:42:54.604711   18944 out.go:270] ! Failing to connect to https://registry.k8s.io/ from inside the minikube container
W0911 12:42:54.606718   18944 out.go:270] * To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0911 12:42:54.640641   18944 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0911 12:42:54.802359   18944 start.go:495] detecting cgroup driver to use...
I0911 12:42:54.802359   18944 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0911 12:42:54.806169   18944 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0911 12:42:54.829676   18944 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0911 12:42:54.834688   18944 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0911 12:42:54.854386   18944 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0911 12:42:54.887327   18944 ssh_runner.go:195] Run: which cri-dockerd
I0911 12:42:54.904582   18944 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0911 12:42:54.934670   18944 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0911 12:42:54.975879   18944 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0911 12:42:55.076002   18944 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0911 12:42:55.174463   18944 docker.go:587] configuring docker to use "cgroupfs" as cgroup driver...
I0911 12:42:55.181127   18944 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0911 12:42:55.214740   18944 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I0911 12:42:55.239967   18944 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0911 12:42:55.321658   18944 ssh_runner.go:195] Run: sudo systemctl restart docker
I0911 12:42:58.014131   18944 ssh_runner.go:235] Completed: sudo systemctl restart docker: (2.6924728s)
I0911 12:42:58.034058   18944 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0911 12:42:58.067292   18944 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0911 12:42:58.091993   18944 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0911 12:42:58.175421   18944 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0911 12:42:58.274432   18944 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0911 12:42:58.352061   18944 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0911 12:42:58.399474   18944 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I0911 12:42:58.428271   18944 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0911 12:42:58.509654   18944 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0911 12:42:59.332124   18944 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0911 12:42:59.366457   18944 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0911 12:42:59.368243   18944 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0911 12:42:59.377596   18944 start.go:563] Will wait 60s for crictl version
I0911 12:42:59.378865   18944 ssh_runner.go:195] Run: which crictl
I0911 12:42:59.391620   18944 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0911 12:42:59.990863   18944 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.1.1
RuntimeApiVersion:  v1
I0911 12:42:59.996388   18944 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0911 12:43:00.550220   18944 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0911 12:43:00.633960   18944 out.go:235] * Preparing Kubernetes v1.33.1 on Docker 28.1.1 ...
I0911 12:43:00.639703   18944 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0911 12:43:00.936636   18944 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0911 12:43:00.939320   18944 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0911 12:43:00.948824   18944 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0911 12:43:00.987422   18944 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0911 12:43:01.061611   18944 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\user:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0911 12:43:01.063376   18944 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0911 12:43:01.068489   18944 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0911 12:43:01.147384   18944 docker.go:702] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.33.1
registry.k8s.io/kube-controller-manager:v1.33.1
registry.k8s.io/kube-scheduler:v1.33.1
registry.k8s.io/kube-proxy:v1.33.1
registry.k8s.io/etcd:3.5.21-0
registry.k8s.io/coredns/coredns:v1.12.0
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0911 12:43:01.147384   18944 docker.go:632] Images already preloaded, skipping extraction
I0911 12:43:01.151762   18944 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0911 12:43:01.186308   18944 docker.go:702] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.33.1
registry.k8s.io/kube-controller-manager:v1.33.1
registry.k8s.io/kube-scheduler:v1.33.1
registry.k8s.io/kube-proxy:v1.33.1
registry.k8s.io/etcd:3.5.21-0
registry.k8s.io/coredns/coredns:v1.12.0
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0911 12:43:01.188001   18944 cache_images.go:84] Images are preloaded, skipping loading
I0911 12:43:01.188001   18944 kubeadm.go:926] updating node { 192.168.49.2 8443 v1.33.1 docker true true} ...
I0911 12:43:01.189809   18944 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.33.1/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0911 12:43:01.194186   18944 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0911 12:43:02.399312   18944 ssh_runner.go:235] Completed: docker info --format {{.CgroupDriver}}: (1.2051266s)
I0911 12:43:02.399838   18944 cni.go:84] Creating CNI manager for ""
I0911 12:43:02.399838   18944 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0911 12:43:02.401004   18944 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0911 12:43:02.401596   18944 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.33.1 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0911 12:43:02.404044   18944 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.33.1
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0911 12:43:02.407713   18944 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.33.1
I0911 12:43:02.430026   18944 binaries.go:44] Found k8s binaries, skipping transfer
I0911 12:43:02.432715   18944 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0911 12:43:02.449006   18944 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0911 12:43:02.481971   18944 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0911 12:43:02.513069   18944 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2286 bytes)
I0911 12:43:02.559709   18944 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0911 12:43:02.570056   18944 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0911 12:43:02.611562   18944 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0911 12:43:02.718091   18944 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0911 12:43:02.767118   18944 certs.go:68] Setting up C:\Users\user\.minikube\profiles\minikube for IP: 192.168.49.2
I0911 12:43:02.767807   18944 certs.go:194] generating shared ca certs ...
I0911 12:43:02.767807   18944 certs.go:226] acquiring lock for ca certs: {Name:mk7164a99e354388efd17e38c9f3aba55425681b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0911 12:43:02.773752   18944 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\user\.minikube\ca.key
I0911 12:43:02.778258   18944 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\user\.minikube\proxy-client-ca.key
I0911 12:43:02.778258   18944 certs.go:256] generating profile certs ...
I0911 12:43:02.780967   18944 certs.go:363] generating signed profile cert for "minikube-user": C:\Users\user\.minikube\profiles\minikube\client.key
I0911 12:43:02.785207   18944 crypto.go:68] Generating cert C:\Users\user\.minikube\profiles\minikube\client.crt with IP's: []
I0911 12:43:03.461027   18944 crypto.go:156] Writing cert to C:\Users\user\.minikube\profiles\minikube\client.crt ...
I0911 12:43:03.461563   18944 lock.go:35] WriteFile acquiring C:\Users\user\.minikube\profiles\minikube\client.crt: {Name:mk0680c065b3554efadcc6595bac227090e1920d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0911 12:43:03.461563   18944 crypto.go:164] Writing key to C:\Users\user\.minikube\profiles\minikube\client.key ...
I0911 12:43:03.461563   18944 lock.go:35] WriteFile acquiring C:\Users\user\.minikube\profiles\minikube\client.key: {Name:mk4ac1bbf8a6de97d3e1c0c9df669ec2bddf95f3 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0911 12:43:03.463074   18944 certs.go:363] generating signed profile cert for "minikube": C:\Users\user\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I0911 12:43:03.463074   18944 crypto.go:68] Generating cert C:\Users\user\.minikube\profiles\minikube\apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I0911 12:43:03.648468   18944 crypto.go:156] Writing cert to C:\Users\user\.minikube\profiles\minikube\apiserver.crt.7fb57e3c ...
I0911 12:43:03.648468   18944 lock.go:35] WriteFile acquiring C:\Users\user\.minikube\profiles\minikube\apiserver.crt.7fb57e3c: {Name:mk0f17f75f9b76a4d6d7e691c7c4a077fb68380b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0911 12:43:03.651777   18944 crypto.go:164] Writing key to C:\Users\user\.minikube\profiles\minikube\apiserver.key.7fb57e3c ...
I0911 12:43:03.651777   18944 lock.go:35] WriteFile acquiring C:\Users\user\.minikube\profiles\minikube\apiserver.key.7fb57e3c: {Name:mkb8c4cba090148132248f8ca829d75998933c2f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0911 12:43:03.654169   18944 certs.go:381] copying C:\Users\user\.minikube\profiles\minikube\apiserver.crt.7fb57e3c -> C:\Users\user\.minikube\profiles\minikube\apiserver.crt
I0911 12:43:03.677311   18944 certs.go:385] copying C:\Users\user\.minikube\profiles\minikube\apiserver.key.7fb57e3c -> C:\Users\user\.minikube\profiles\minikube\apiserver.key
I0911 12:43:03.680028   18944 certs.go:363] generating signed profile cert for "aggregator": C:\Users\user\.minikube\profiles\minikube\proxy-client.key
I0911 12:43:03.680028   18944 crypto.go:68] Generating cert C:\Users\user\.minikube\profiles\minikube\proxy-client.crt with IP's: []
I0911 12:43:03.954148   18944 crypto.go:156] Writing cert to C:\Users\user\.minikube\profiles\minikube\proxy-client.crt ...
I0911 12:43:03.954148   18944 lock.go:35] WriteFile acquiring C:\Users\user\.minikube\profiles\minikube\proxy-client.crt: {Name:mk200539a6a0695144ec612df6c156d7098533e4 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0911 12:43:03.963054   18944 crypto.go:164] Writing key to C:\Users\user\.minikube\profiles\minikube\proxy-client.key ...
I0911 12:43:03.963054   18944 lock.go:35] WriteFile acquiring C:\Users\user\.minikube\profiles\minikube\proxy-client.key: {Name:mk9d85596795208b222d85d0a551532bb58b1dcb Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0911 12:43:03.984740   18944 certs.go:484] found cert: C:\Users\user\.minikube\certs\ca-key.pem (1675 bytes)
I0911 12:43:03.985341   18944 certs.go:484] found cert: C:\Users\user\.minikube\certs\ca.pem (1070 bytes)
I0911 12:43:03.985341   18944 certs.go:484] found cert: C:\Users\user\.minikube\certs\cert.pem (1115 bytes)
I0911 12:43:03.985930   18944 certs.go:484] found cert: C:\Users\user\.minikube\certs\key.pem (1675 bytes)
I0911 12:43:04.018138   18944 ssh_runner.go:362] scp C:\Users\user\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0911 12:43:04.057909   18944 ssh_runner.go:362] scp C:\Users\user\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0911 12:43:04.091717   18944 ssh_runner.go:362] scp C:\Users\user\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0911 12:43:04.125606   18944 ssh_runner.go:362] scp C:\Users\user\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0911 12:43:04.159929   18944 ssh_runner.go:362] scp C:\Users\user\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0911 12:43:04.193826   18944 ssh_runner.go:362] scp C:\Users\user\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0911 12:43:04.240115   18944 ssh_runner.go:362] scp C:\Users\user\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0911 12:43:04.278839   18944 ssh_runner.go:362] scp C:\Users\user\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0911 12:43:04.320362   18944 ssh_runner.go:362] scp C:\Users\user\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0911 12:43:04.355679   18944 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0911 12:43:04.380895   18944 ssh_runner.go:195] Run: openssl version
I0911 12:43:04.399491   18944 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0911 12:43:04.417874   18944 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0911 12:43:04.427063   18944 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Sep  9 06:25 /usr/share/ca-certificates/minikubeCA.pem
I0911 12:43:04.427825   18944 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0911 12:43:04.455275   18944 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0911 12:43:04.469710   18944 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0911 12:43:04.481831   18944 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0911 12:43:04.481831   18944 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\user:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0911 12:43:04.486616   18944 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0911 12:43:04.537271   18944 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0911 12:43:04.564189   18944 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0911 12:43:04.580996   18944 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I0911 12:43:04.587500   18944 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0911 12:43:04.604460   18944 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0911 12:43:04.604460   18944 kubeadm.go:157] found existing configuration files:

I0911 12:43:04.610811   18944 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0911 12:43:04.630240   18944 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0911 12:43:04.636214   18944 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0911 12:43:04.667569   18944 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0911 12:43:04.687544   18944 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0911 12:43:04.695234   18944 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0911 12:43:04.720647   18944 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0911 12:43:04.757385   18944 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0911 12:43:04.759984   18944 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0911 12:43:04.775644   18944 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0911 12:43:04.787767   18944 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0911 12:43:04.791217   18944 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0911 12:43:04.805377   18944 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.33.1:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0911 12:43:05.545682   18944 kubeadm.go:310] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I0911 12:43:05.775082   18944 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0911 12:43:21.275316   18944 kubeadm.go:310] [init] Using Kubernetes version: v1.33.1
I0911 12:43:21.275316   18944 kubeadm.go:310] [preflight] Running pre-flight checks
I0911 12:43:21.275823   18944 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I0911 12:43:21.275823   18944 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0911 12:43:21.275823   18944 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I0911 12:43:21.275823   18944 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0911 12:43:21.276949   18944 out.go:235]   - Generating certificates and keys ...
I0911 12:43:21.278054   18944 kubeadm.go:310] [certs] Using existing ca certificate authority
I0911 12:43:21.278054   18944 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I0911 12:43:21.278054   18944 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I0911 12:43:21.278054   18944 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I0911 12:43:21.278614   18944 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I0911 12:43:21.278614   18944 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I0911 12:43:21.278614   18944 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I0911 12:43:21.278614   18944 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0911 12:43:21.278614   18944 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I0911 12:43:21.279159   18944 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0911 12:43:21.279159   18944 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I0911 12:43:21.279159   18944 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I0911 12:43:21.279159   18944 kubeadm.go:310] [certs] Generating "sa" key and public key
I0911 12:43:21.279696   18944 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0911 12:43:21.279696   18944 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I0911 12:43:21.279696   18944 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0911 12:43:21.279696   18944 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0911 12:43:21.279696   18944 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0911 12:43:21.279696   18944 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0911 12:43:21.279696   18944 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0911 12:43:21.280255   18944 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0911 12:43:21.280255   18944 out.go:235]   - Booting up control plane ...
I0911 12:43:21.281587   18944 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0911 12:43:21.281587   18944 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0911 12:43:21.281587   18944 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0911 12:43:21.281587   18944 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0911 12:43:21.282121   18944 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0911 12:43:21.282121   18944 kubeadm.go:310] [kubelet-start] Starting the kubelet
I0911 12:43:21.282121   18944 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0911 12:43:21.282121   18944 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I0911 12:43:21.282653   18944 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 1.005043803s
I0911 12:43:21.282653   18944 kubeadm.go:310] [control-plane-check] Waiting for healthy control plane components. This can take up to 4m0s
I0911 12:43:21.282653   18944 kubeadm.go:310] [control-plane-check] Checking kube-apiserver at https://192.168.49.2:8443/livez
I0911 12:43:21.282653   18944 kubeadm.go:310] [control-plane-check] Checking kube-controller-manager at https://127.0.0.1:10257/healthz
I0911 12:43:21.283177   18944 kubeadm.go:310] [control-plane-check] Checking kube-scheduler at https://127.0.0.1:10259/livez
I0911 12:43:21.283177   18944 kubeadm.go:310] [control-plane-check] kube-controller-manager is healthy after 5.849220287s
I0911 12:43:21.283177   18944 kubeadm.go:310] [control-plane-check] kube-scheduler is healthy after 7.915293049s
I0911 12:43:21.283177   18944 kubeadm.go:310] [control-plane-check] kube-apiserver is healthy after 10.002846882s
I0911 12:43:21.283710   18944 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0911 12:43:21.283710   18944 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0911 12:43:21.283710   18944 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I0911 12:43:21.283710   18944 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0911 12:43:21.284255   18944 kubeadm.go:310] [bootstrap-token] Using token: mfk5z6.mjb1uxt96gbvsggv
I0911 12:43:21.284806   18944 out.go:235]   - Configuring RBAC rules ...
I0911 12:43:21.284806   18944 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0911 12:43:21.285334   18944 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0911 12:43:21.285334   18944 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0911 12:43:21.285334   18944 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0911 12:43:21.285334   18944 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0911 12:43:21.285882   18944 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0911 12:43:21.285882   18944 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0911 12:43:21.285882   18944 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I0911 12:43:21.285882   18944 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I0911 12:43:21.285882   18944 kubeadm.go:310] 
I0911 12:43:21.285882   18944 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I0911 12:43:21.285882   18944 kubeadm.go:310] 
I0911 12:43:21.285882   18944 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I0911 12:43:21.285882   18944 kubeadm.go:310] 
I0911 12:43:21.285882   18944 kubeadm.go:310]   mkdir -p $HOME/.kube
I0911 12:43:21.286433   18944 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0911 12:43:21.286433   18944 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0911 12:43:21.286433   18944 kubeadm.go:310] 
I0911 12:43:21.286433   18944 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I0911 12:43:21.286433   18944 kubeadm.go:310] 
I0911 12:43:21.286433   18944 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0911 12:43:21.286433   18944 kubeadm.go:310] 
I0911 12:43:21.286433   18944 kubeadm.go:310] You should now deploy a pod network to the cluster.
I0911 12:43:21.286433   18944 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0911 12:43:21.286971   18944 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0911 12:43:21.286971   18944 kubeadm.go:310] 
I0911 12:43:21.286971   18944 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I0911 12:43:21.286971   18944 kubeadm.go:310] and service account keys on each node and then running the following as root:
I0911 12:43:21.286971   18944 kubeadm.go:310] 
I0911 12:43:21.286971   18944 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token mfk5z6.mjb1uxt96gbvsggv \
I0911 12:43:21.286971   18944 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:a6936c3a395ba3106628b5cd87cc69429bd882b3ce083a344b2031409f95a842 \
I0911 12:43:21.286971   18944 kubeadm.go:310] 	--control-plane 
I0911 12:43:21.286971   18944 kubeadm.go:310] 
I0911 12:43:21.287510   18944 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I0911 12:43:21.287510   18944 kubeadm.go:310] 
I0911 12:43:21.287510   18944 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token mfk5z6.mjb1uxt96gbvsggv \
I0911 12:43:21.287510   18944 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:a6936c3a395ba3106628b5cd87cc69429bd882b3ce083a344b2031409f95a842 
I0911 12:43:21.287510   18944 cni.go:84] Creating CNI manager for ""
I0911 12:43:21.287510   18944 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0911 12:43:21.288648   18944 out.go:177] * Configuring bridge CNI (Container Networking Interface) ...
I0911 12:43:21.296879   18944 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0911 12:43:21.389075   18944 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0911 12:43:21.502711   18944 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0911 12:43:21.511237   18944 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_09_11T12_43_21_0700 minikube.k8s.io/version=v1.36.0 minikube.k8s.io/commit=f8f52f5de11fc6ad8244afac475e1d0f96841df1-dirty minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0911 12:43:21.512340   18944 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.33.1/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0911 12:43:21.560556   18944 ops.go:34] apiserver oom_adj: -16
I0911 12:43:23.114644   18944 ssh_runner.go:235] Completed: sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_09_11T12_43_21_0700 minikube.k8s.io/version=v1.36.0 minikube.k8s.io/commit=f8f52f5de11fc6ad8244afac475e1d0f96841df1-dirty minikube.k8s.io/name=minikube minikube.k8s.io/primary=true: (1.6034077s)
I0911 12:43:23.179847   18944 ssh_runner.go:235] Completed: sudo /var/lib/minikube/binaries/v1.33.1/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig: (1.6675077s)
I0911 12:43:23.179847   18944 kubeadm.go:1105] duration metric: took 1.6758478s to wait for elevateKubeSystemPrivileges
I0911 12:43:23.179847   18944 kubeadm.go:394] duration metric: took 18.6980166s to StartCluster
I0911 12:43:23.180432   18944 settings.go:142] acquiring lock: {Name:mkeb1fd696d13c02454084a505541301abc7d400 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0911 12:43:23.180432   18944 settings.go:150] Updating kubeconfig:  C:\Users\user\.kube\config
I0911 12:43:23.196424   18944 lock.go:35] WriteFile acquiring C:\Users\user\.kube\config: {Name:mkf0568b8c360d314f1ebed2036702ffdf9396af Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0911 12:43:23.199249   18944 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}
I0911 12:43:23.199249   18944 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0911 12:43:23.199249   18944 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0911 12:43:23.200481   18944 out.go:177] * Verifying Kubernetes components...
I0911 12:43:23.200481   18944 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0911 12:43:23.202202   18944 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0911 12:43:23.202202   18944 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0911 12:43:23.203609   18944 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0911 12:43:23.203609   18944 addons.go:238] Setting addon storage-provisioner=true in "minikube"
I0911 12:43:23.205356   18944 host.go:66] Checking if "minikube" exists ...
I0911 12:43:23.209019   18944 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0911 12:43:23.230357   18944 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0911 12:43:23.231384   18944 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0911 12:43:23.304936   18944 out.go:177]   - Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0911 12:43:23.307218   18944 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0911 12:43:23.307218   18944 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0911 12:43:23.316378   18944 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0911 12:43:23.363899   18944 addons.go:238] Setting addon default-storageclass=true in "minikube"
I0911 12:43:23.364437   18944 host.go:66] Checking if "minikube" exists ...
I0911 12:43:23.387715   18944 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:60002 SSHKeyPath:C:\Users\user\.minikube\machines\minikube\id_rsa Username:docker}
I0911 12:43:23.390088   18944 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0911 12:43:23.451690   18944 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0911 12:43:23.451690   18944 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0911 12:43:23.460386   18944 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0911 12:43:23.472461   18944 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.254 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0911 12:43:23.475176   18944 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0911 12:43:23.531687   18944 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:60002 SSHKeyPath:C:\Users\user\.minikube\machines\minikube\id_rsa Username:docker}
I0911 12:43:23.671157   18944 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0911 12:43:23.975812   18944 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0911 12:43:24.763349   18944 ssh_runner.go:235] Completed: sudo systemctl start kubelet: (1.2881733s)
I0911 12:43:24.763860   18944 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.254 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -": (1.2913989s)
I0911 12:43:24.763860   18944 start.go:971] {"host.minikube.internal": 192.168.65.254} host record injected into CoreDNS's ConfigMap
I0911 12:43:24.777604   18944 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0911 12:43:24.853697   18944 api_server.go:52] waiting for apiserver process to appear ...
I0911 12:43:24.864314   18944 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0911 12:43:25.283358   18944 kapi.go:214] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0911 12:43:25.355653   18944 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.6844959s)
I0911 12:43:25.355653   18944 api_server.go:72] duration metric: took 2.156404s to wait for apiserver process to appear ...
I0911 12:43:25.355653   18944 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (1.3798408s)
I0911 12:43:25.355653   18944 api_server.go:88] waiting for apiserver healthz status ...
I0911 12:43:25.356767   18944 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:60001/healthz ...
I0911 12:43:25.378389   18944 api_server.go:279] https://127.0.0.1:60001/healthz returned 200:
ok
I0911 12:43:25.384875   18944 api_server.go:141] control plane version: v1.33.1
I0911 12:43:25.384875   18944 api_server.go:131] duration metric: took 28.6642ms to wait for apiserver health ...
I0911 12:43:25.385414   18944 system_pods.go:43] waiting for kube-system pods to appear ...
I0911 12:43:25.391715   18944 out.go:177] * Enabled addons: storage-provisioner, default-storageclass
I0911 12:43:25.392252   18944 addons.go:514] duration metric: took 2.1930028s for enable addons: enabled=[storage-provisioner default-storageclass]
I0911 12:43:25.403741   18944 system_pods.go:59] 5 kube-system pods found
I0911 12:43:25.404268   18944 system_pods.go:61] "etcd-minikube" [74a5fc92-a7d7-4f65-a84a-eafe8b5b3393] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0911 12:43:25.404268   18944 system_pods.go:61] "kube-apiserver-minikube" [37739207-9c58-4655-90fd-b1caef94148a] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0911 12:43:25.404268   18944 system_pods.go:61] "kube-controller-manager-minikube" [2be91d9a-d58c-4dcc-8329-e0405315e7d6] Running
I0911 12:43:25.404268   18944 system_pods.go:61] "kube-scheduler-minikube" [322d8c67-6f30-4586-864f-1877ce076ba2] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0911 12:43:25.404268   18944 system_pods.go:61] "storage-provisioner" [25dfe268-1775-4785-b617-b29199fd3012] Pending
I0911 12:43:25.404268   18944 system_pods.go:74] duration metric: took 18.8544ms to wait for pod list to return data ...
I0911 12:43:25.404268   18944 kubeadm.go:578] duration metric: took 2.2050189s to wait for: map[apiserver:true system_pods:true]
I0911 12:43:25.404268   18944 node_conditions.go:102] verifying NodePressure condition ...
I0911 12:43:25.455229   18944 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0911 12:43:25.456332   18944 node_conditions.go:123] node cpu capacity is 6
I0911 12:43:25.457508   18944 node_conditions.go:105] duration metric: took 53.2394ms to run NodePressure ...
I0911 12:43:25.457508   18944 start.go:241] waiting for startup goroutines ...
I0911 12:43:25.457508   18944 start.go:246] waiting for cluster config update ...
I0911 12:43:25.457508   18944 start.go:255] writing updated cluster config ...
I0911 12:43:25.462561   18944 ssh_runner.go:195] Run: rm -f paused
I0911 12:43:25.646111   18944 start.go:607] kubectl: 1.32.2, cluster: 1.33.1 (minor skew: 1)
I0911 12:43:25.647255   18944 out.go:177] * Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Sep 11 15:23:59 minikube dockerd[1320]: time="2025-09-11T15:23:59.144265996Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 11 15:23:59 minikube dockerd[1320]: time="2025-09-11T15:23:59.144651940Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 11 15:24:09 minikube dockerd[1320]: time="2025-09-11T15:24:09.914268763Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 11 15:24:09 minikube dockerd[1320]: time="2025-09-11T15:24:09.914368435Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 11 15:24:14 minikube dockerd[1320]: time="2025-09-11T15:24:14.158219008Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 11 15:24:14 minikube dockerd[1320]: time="2025-09-11T15:24:14.158329970Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 11 15:25:23 minikube dockerd[1320]: time="2025-09-11T15:25:23.143409120Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 11 15:25:23 minikube dockerd[1320]: time="2025-09-11T15:25:23.146763411Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 11 15:25:41 minikube dockerd[1320]: time="2025-09-11T15:25:41.048264223Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 11 15:25:41 minikube dockerd[1320]: time="2025-09-11T15:25:41.048316591Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 11 15:25:44 minikube dockerd[1320]: time="2025-09-11T15:25:44.590958154Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 11 15:25:44 minikube dockerd[1320]: time="2025-09-11T15:25:44.591082417Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 11 15:28:17 minikube dockerd[1320]: time="2025-09-11T15:28:17.008621565Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 11 15:28:17 minikube dockerd[1320]: time="2025-09-11T15:28:17.009275751Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 11 15:28:24 minikube dockerd[1320]: time="2025-09-11T15:28:24.975244871Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 11 15:28:24 minikube dockerd[1320]: time="2025-09-11T15:28:24.975361782Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 11 15:28:31 minikube dockerd[1320]: time="2025-09-11T15:28:31.132153787Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 11 15:28:31 minikube dockerd[1320]: time="2025-09-11T15:28:31.132265641Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 11 15:33:22 minikube dockerd[1320]: time="2025-09-11T15:33:22.135282587Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 11 15:33:22 minikube dockerd[1320]: time="2025-09-11T15:33:22.139849848Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 11 15:33:40 minikube dockerd[1320]: time="2025-09-11T15:33:40.686796678Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 11 15:33:40 minikube dockerd[1320]: time="2025-09-11T15:33:40.686869103Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 11 15:33:44 minikube dockerd[1320]: time="2025-09-11T15:33:44.272378504Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 11 15:33:44 minikube dockerd[1320]: time="2025-09-11T15:33:44.272526231Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 11 15:36:42 minikube dockerd[1320]: time="2025-09-11T15:36:42.844650336Z" level=info msg="ignoring event" container=3a357fc9c56b445e6c27657008f4b6d4ebc63a4e8f78da7c4bb328592d89ea17 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 11 15:36:42 minikube dockerd[1320]: time="2025-09-11T15:36:42.896969304Z" level=info msg="ignoring event" container=2e09dabf18c67a6a168c185ed115574aae468f758041a54de1086c26c2b26541 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 11 15:36:42 minikube dockerd[1320]: time="2025-09-11T15:36:42.935531743Z" level=info msg="ignoring event" container=1db5e71ca684a59d02cb2ca288353ff9d72445ecb1341b2b6d1a1e90660d679e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 11 15:37:09 minikube cri-dockerd[1628]: time="2025-09-11T15:37:09Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a7c11f6a7e60b0fbb687e150bee08df2ac06ae09a269cd9759707314bbf64939/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 11 15:37:09 minikube cri-dockerd[1628]: time="2025-09-11T15:37:09Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/37abd63efb7edd86ba3e39900397b6850a6ba77983dbc49902fc6d48b79e0d93/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 11 15:37:09 minikube cri-dockerd[1628]: time="2025-09-11T15:37:09Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/836c3a791a27e736d00d771573cbd3178ce17bc9b050f13ec3a28ffa97b2795f/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 11 15:37:12 minikube dockerd[1320]: time="2025-09-11T15:37:12.911064233Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 11 15:37:12 minikube dockerd[1320]: time="2025-09-11T15:37:12.911550169Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 11 15:37:16 minikube dockerd[1320]: time="2025-09-11T15:37:16.271980936Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 11 15:37:16 minikube dockerd[1320]: time="2025-09-11T15:37:16.272017277Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 11 15:37:19 minikube dockerd[1320]: time="2025-09-11T15:37:19.688776916Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 11 15:37:19 minikube dockerd[1320]: time="2025-09-11T15:37:19.688883948Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 11 15:37:30 minikube dockerd[1320]: time="2025-09-11T15:37:30.984085979Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 11 15:37:30 minikube dockerd[1320]: time="2025-09-11T15:37:30.984270507Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 11 15:37:34 minikube dockerd[1320]: time="2025-09-11T15:37:34.721693943Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 11 15:37:34 minikube dockerd[1320]: time="2025-09-11T15:37:34.721768088Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 11 15:37:38 minikube dockerd[1320]: time="2025-09-11T15:37:38.250975004Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 11 15:37:38 minikube dockerd[1320]: time="2025-09-11T15:37:38.251042946Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 11 15:38:00 minikube dockerd[1320]: time="2025-09-11T15:38:00.073234013Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 11 15:38:00 minikube dockerd[1320]: time="2025-09-11T15:38:00.073436749Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 11 15:38:06 minikube dockerd[1320]: time="2025-09-11T15:38:06.968877958Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 11 15:38:06 minikube dockerd[1320]: time="2025-09-11T15:38:06.968930925Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 11 15:38:10 minikube dockerd[1320]: time="2025-09-11T15:38:10.357555776Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 11 15:38:10 minikube dockerd[1320]: time="2025-09-11T15:38:10.357705088Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 11 15:38:43 minikube dockerd[1320]: time="2025-09-11T15:38:43.992370320Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 11 15:38:43 minikube dockerd[1320]: time="2025-09-11T15:38:43.992430069Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 11 15:39:00 minikube dockerd[1320]: time="2025-09-11T15:39:00.929586573Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 11 15:39:00 minikube dockerd[1320]: time="2025-09-11T15:39:00.929692794Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 11 15:39:04 minikube dockerd[1320]: time="2025-09-11T15:39:04.472505346Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 11 15:39:04 minikube dockerd[1320]: time="2025-09-11T15:39:04.472584903Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 11 15:40:09 minikube dockerd[1320]: time="2025-09-11T15:40:09.090009043Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 11 15:40:09 minikube dockerd[1320]: time="2025-09-11T15:40:09.090104792Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 11 15:40:29 minikube dockerd[1320]: time="2025-09-11T15:40:29.057008752Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 11 15:40:29 minikube dockerd[1320]: time="2025-09-11T15:40:29.057158602Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Sep 11 15:40:34 minikube dockerd[1320]: time="2025-09-11T15:40:34.991517356Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Sep 11 15:40:34 minikube dockerd[1320]: time="2025-09-11T15:40:34.991597257Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
f5c06664425d5       6e38f40d628db       51 minutes ago      Running             storage-provisioner       2                   f78ef33229751       storage-provisioner
644c6a8bc3e4e       6e38f40d628db       8 hours ago         Exited              storage-provisioner       1                   f78ef33229751       storage-provisioner
e900d8dd77e26       1cf5f116067c6       8 hours ago         Running             coredns                   0                   d1428ec67a65d       coredns-674b8bbfcf-6hghm
2975ecdf46039       b79c189b052cd       8 hours ago         Running             kube-proxy                0                   065d77231c4d7       kube-proxy-sgxkz
7790a1c4bb4a9       398c985c0d950       8 hours ago         Running             kube-scheduler            0                   9d7849d26d835       kube-scheduler-minikube
a7e9d75f2ce6a       c6ab243b29f82       8 hours ago         Running             kube-apiserver            0                   79028458e52bc       kube-apiserver-minikube
807b054bd50c0       499038711c081       8 hours ago         Running             etcd                      0                   bd33c7e1474a9       etcd-minikube
7891befb3ea24       ef43894fa110c       8 hours ago         Running             kube-controller-manager   0                   3cd944c7b1254       kube-controller-manager-minikube


==> coredns [e900d8dd77e2] <==
maxprocs: Leaving GOMAXPROCS=6: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.12.0
linux/amd64, go1.23.3, 51e11f1
[INFO] 127.0.0.1:43880 - 27680 "HINFO IN 7923330475807672308.8479003061998334937. udp 57 false 512" NXDOMAIN qr,rd,ra 57 2.017843827s
[INFO] 127.0.0.1:47945 - 18218 "HINFO IN 7923330475807672308.8479003061998334937. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.007654148s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[ERROR] plugin/kubernetes: Unhandled Error
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.510170317s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.99011544s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=f8f52f5de11fc6ad8244afac475e1d0f96841df1-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_09_11T12_43_21_0700
                    minikube.k8s.io/version=v1.36.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 11 Sep 2025 07:13:17 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Thu, 11 Sep 2025 15:42:04 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Thu, 11 Sep 2025 15:40:04 +0000   Thu, 11 Sep 2025 07:13:12 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Thu, 11 Sep 2025 15:40:04 +0000   Thu, 11 Sep 2025 07:13:12 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Thu, 11 Sep 2025 15:40:04 +0000   Thu, 11 Sep 2025 07:13:12 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Thu, 11 Sep 2025 15:40:04 +0000   Thu, 11 Sep 2025 07:13:17 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                6
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3850984Ki
  pods:               110
Allocatable:
  cpu:                6
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3850984Ki
  pods:               110
System Info:
  Machine ID:                 0c2876cdbab5447d8737f4de5517afb3
  System UUID:                0c2876cdbab5447d8737f4de5517afb3
  Boot ID:                    57a242be-f004-48e6-a3ad-044a03b12274
  Kernel Version:             6.6.87.2-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://28.1.1
  Kubelet Version:            v1.33.1
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (10 in total)
  Namespace                   Name                                   CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                   ------------  ----------  ---------------  -------------  ---
  default                     springboot-app-ek8-5d8654f8c9-dgt9d    0 (0%)        0 (0%)      0 (0%)           0 (0%)         5m7s
  default                     springboot-app-ek8-5d8654f8c9-k2fc2    0 (0%)        0 (0%)      0 (0%)           0 (0%)         5m7s
  default                     springboot-app-ek8-5d8654f8c9-xmsxv    0 (0%)        0 (0%)      0 (0%)           0 (0%)         5m7s
  kube-system                 coredns-674b8bbfcf-6hghm               100m (1%)     0 (0%)      70Mi (1%)        170Mi (4%)     8h
  kube-system                 etcd-minikube                          100m (1%)     0 (0%)      100Mi (2%)       0 (0%)         8h
  kube-system                 kube-apiserver-minikube                250m (4%)     0 (0%)      0 (0%)           0 (0%)         8h
  kube-system                 kube-controller-manager-minikube       200m (3%)     0 (0%)      0 (0%)           0 (0%)         8h
  kube-system                 kube-proxy-sgxkz                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         8h
  kube-system                 kube-scheduler-minikube                100m (1%)     0 (0%)      0 (0%)           0 (0%)         8h
  kube-system                 storage-provisioner                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         8h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (12%)  0 (0%)
  memory             170Mi (4%)  170Mi (4%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type    Reason                   Age   From             Message
  ----    ------                   ----  ----             -------
  Normal  Starting                 8h    kube-proxy       
  Normal  Starting                 8h    kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  8h    kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  8h    kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    8h    kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     8h    kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           8h    node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Sep11 13:12] Hyper-V: Disabling IBT because of Hyper-V bug
[  +0.156672] PCI: Fatal: No config space access function found
[  +0.039914] PCI: System does not support PCI
[  +0.433567] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +1.659492] /dev/sda: Can't lookup blockdev
[  +3.815182] WSL (1 - init(docker-desktop)) ERROR: ConfigApplyWindowsLibPath:2119: open /etc/ld.so.conf.d/ld.wsl.conf failed 2
[  +0.028396] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[  +0.293427] pulseaudio[257]: memfd_create() called without MFD_EXEC or MFD_NOEXEC_SEAL set
[  +0.747328] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.042267] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.007571] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.034281] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001860] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +1.611320] netlink: 'init': attribute type 4 has an invalid length.
[  +1.065698] WSL (205) ERROR: CheckConnection: getaddrinfo() failed: -5
[Sep11 13:13] ICMPv6: NA: c2:79:00:04:00:d6 advertised our address fc00:f853:ccd:e793::2 on eth0!
[  +0.064140] ICMPv6: NA: c2:2b:ae:f1:c6:71 advertised our address fc00:f853:ccd:e793::4 on eth0!
[  +0.001554] ICMPv6: NA: 26:6d:b8:7e:ef:32 advertised our address fc00:f853:ccd:e793::3 on eth0!
[  +0.938774] ICMPv6: NA: c2:79:00:04:00:d6 advertised our address fc00:f853:ccd:e793::2 on eth0!
[  +0.061954] ICMPv6: NA: c2:2b:ae:f1:c6:71 advertised our address fc00:f853:ccd:e793::4 on eth0!
[  +0.005125] ICMPv6: NA: 26:6d:b8:7e:ef:32 advertised our address fc00:f853:ccd:e793::3 on eth0!
[  +0.933797] ICMPv6: NA: c2:79:00:04:00:d6 advertised our address fc00:f853:ccd:e793::2 on eth0!
[  +0.061543] ICMPv6: NA: c2:2b:ae:f1:c6:71 advertised our address fc00:f853:ccd:e793::4 on eth0!
[  +0.000157] ICMPv6: NA: 26:6d:b8:7e:ef:32 advertised our address fc00:f853:ccd:e793::3 on eth0!
[ +13.001384] ICMPv6: NA: ce:ee:6e:10:3a:d1 advertised our address fc00:f853:ccd:e793::2 on eth0!
[  +1.002541] ICMPv6: NA: ce:ee:6e:10:3a:d1 advertised our address fc00:f853:ccd:e793::2 on eth0!
[  +0.999965] ICMPv6: NA: ce:ee:6e:10:3a:d1 advertised our address fc00:f853:ccd:e793::2 on eth0!
[ +26.495702] ICMPv6: NA: ae:9b:3b:44:df:c4 advertised our address fc00:f853:ccd:e793::3 on eth0!
[  +1.003023] ICMPv6: NA: ae:9b:3b:44:df:c4 advertised our address fc00:f853:ccd:e793::3 on eth0!
[  +0.998766] ICMPv6: NA: ae:9b:3b:44:df:c4 advertised our address fc00:f853:ccd:e793::3 on eth0!
[Sep11 13:14] ICMPv6: NA: ee:a2:c3:7a:c8:42 advertised our address fc00:f853:ccd:e793::4 on eth0!
[  +1.002761] ICMPv6: NA: ee:a2:c3:7a:c8:42 advertised our address fc00:f853:ccd:e793::4 on eth0!
[  +0.999825] ICMPv6: NA: ee:a2:c3:7a:c8:42 advertised our address fc00:f853:ccd:e793::4 on eth0!
[Sep11 14:21] WSL (205) ERROR: CheckConnection: getaddrinfo() failed: -5
[ +27.894537] WSL (205) ERROR: CheckConnection: getaddrinfo() failed: -5
[Sep11 14:22] hrtimer: interrupt took 9865972 ns
[Sep11 14:50] WSL (205) ERROR: CheckConnection: getaddrinfo() failed: -5


==> etcd [807b054bd50c] <==
{"level":"info","ts":"2025-09-11T14:51:22.467641Z","caller":"traceutil/trace.go:171","msg":"trace[60989803] transaction","detail":"{read_only:false; response_revision:598; number_of_response:1; }","duration":"1.938660376s","start":"2025-09-11T14:51:20.528971Z","end":"2025-09-11T14:51:22.467631Z","steps":["trace[60989803] 'process raft request'  (duration: 1.93836747s)"],"step_count":1}
{"level":"warn","ts":"2025-09-11T14:51:22.471854Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-09-11T14:51:20.528682Z","time spent":"1.939055854s","remote":"127.0.0.1:57758","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":797,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/events/default/springboot-single-endpoint-eks-5f6c6d574d-xgf48.18642901640dc1aa\" mod_revision:528 > success:<request_put:<key:\"/registry/events/default/springboot-single-endpoint-eks-5f6c6d574d-xgf48.18642901640dc1aa\" value_size:690 lease:8128039891666958449 >> failure:<request_range:<key:\"/registry/events/default/springboot-single-endpoint-eks-5f6c6d574d-xgf48.18642901640dc1aa\" > >"}
{"level":"warn","ts":"2025-09-11T14:51:22.472822Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"2.000246103s","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-09-11T14:51:22.473116Z","caller":"traceutil/trace.go:171","msg":"trace[135718528] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:598; }","duration":"2.000545129s","start":"2025-09-11T14:51:20.472427Z","end":"2025-09-11T14:51:22.472972Z","steps":["trace[135718528] 'agreement among raft nodes before linearized reading'  (duration: 1.99528607s)"],"step_count":1}
{"level":"info","ts":"2025-09-11T14:51:22.467456Z","caller":"traceutil/trace.go:171","msg":"trace[1223245851] linearizableReadLoop","detail":"{readStateIndex:643; appliedIndex:642; }","duration":"1.994767517s","start":"2025-09-11T14:51:20.472438Z","end":"2025-09-11T14:51:22.467205Z","steps":["trace[1223245851] 'read index received'  (duration: 20.609847ms)","trace[1223245851] 'applied index is now lower than readState.Index'  (duration: 1.974154418s)"],"step_count":2}
{"level":"info","ts":"2025-09-11T14:55:28.932105Z","caller":"traceutil/trace.go:171","msg":"trace[656575075] transaction","detail":"{read_only:false; response_revision:899; number_of_response:1; }","duration":"114.124889ms","start":"2025-09-11T14:55:28.815571Z","end":"2025-09-11T14:55:28.929696Z","steps":["trace[656575075] 'process raft request'  (duration: 113.511791ms)"],"step_count":1}
{"level":"info","ts":"2025-09-11T14:58:43.301930Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":788}
{"level":"info","ts":"2025-09-11T14:58:43.489990Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":788,"took":"175.435444ms","hash":1263716082,"current-db-size-bytes":2531328,"current-db-size":"2.5 MB","current-db-size-in-use-bytes":2531328,"current-db-size-in-use":"2.5 MB"}
{"level":"info","ts":"2025-09-11T14:58:43.492518Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1263716082,"revision":788,"compact-revision":-1}
{"level":"info","ts":"2025-09-11T15:03:43.276616Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1088}
{"level":"info","ts":"2025-09-11T15:03:43.383262Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":1088,"took":"90.836237ms","hash":719428678,"current-db-size-bytes":2531328,"current-db-size":"2.5 MB","current-db-size-in-use-bytes":2072576,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2025-09-11T15:03:43.385499Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":719428678,"revision":1088,"compact-revision":788}
{"level":"info","ts":"2025-09-11T15:08:43.247037Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1465}
{"level":"info","ts":"2025-09-11T15:08:43.306492Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":1465,"took":"54.920321ms","hash":1569308779,"current-db-size-bytes":2531328,"current-db-size":"2.5 MB","current-db-size-in-use-bytes":2203648,"current-db-size-in-use":"2.2 MB"}
{"level":"info","ts":"2025-09-11T15:08:43.306613Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1569308779,"revision":1465,"compact-revision":1088}
{"level":"info","ts":"2025-09-11T15:13:43.230731Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1797}
{"level":"info","ts":"2025-09-11T15:13:43.282033Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":1797,"took":"44.05244ms","hash":891089971,"current-db-size-bytes":2531328,"current-db-size":"2.5 MB","current-db-size-in-use-bytes":1875968,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-09-11T15:13:43.282248Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":891089971,"revision":1797,"compact-revision":1465}
{"level":"info","ts":"2025-09-11T15:18:43.176930Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2049}
{"level":"info","ts":"2025-09-11T15:18:43.208370Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":2049,"took":"26.070175ms","hash":3450915330,"current-db-size-bytes":2531328,"current-db-size":"2.5 MB","current-db-size-in-use-bytes":1748992,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-09-11T15:18:43.209093Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3450915330,"revision":2049,"compact-revision":1797}
{"level":"info","ts":"2025-09-11T15:23:43.132591Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2328}
{"level":"info","ts":"2025-09-11T15:23:43.197631Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":2328,"took":"51.513781ms","hash":998772468,"current-db-size-bytes":2531328,"current-db-size":"2.5 MB","current-db-size-in-use-bytes":1949696,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-09-11T15:23:43.197742Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":998772468,"revision":2328,"compact-revision":2049}
{"level":"info","ts":"2025-09-11T15:28:43.096186Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2664}
{"level":"info","ts":"2025-09-11T15:28:43.173271Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":2664,"took":"64.753375ms","hash":1755997213,"current-db-size-bytes":2531328,"current-db-size":"2.5 MB","current-db-size-in-use-bytes":2031616,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2025-09-11T15:28:43.173387Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1755997213,"revision":2664,"compact-revision":2328}
{"level":"info","ts":"2025-09-11T15:29:14.927533Z","caller":"traceutil/trace.go:171","msg":"trace[575459515] linearizableReadLoop","detail":"{readStateIndex:3584; appliedIndex:3583; }","duration":"108.767858ms","start":"2025-09-11T15:29:14.796813Z","end":"2025-09-11T15:29:14.905581Z","steps":["trace[575459515] 'read index received'  (duration: 108.391606ms)","trace[575459515] 'applied index is now lower than readState.Index'  (duration: 375.642Âµs)"],"step_count":2}
{"level":"info","ts":"2025-09-11T15:29:14.950905Z","caller":"traceutil/trace.go:171","msg":"trace[1853430108] transaction","detail":"{read_only:false; response_revision:3054; number_of_response:1; }","duration":"318.191616ms","start":"2025-09-11T15:29:14.632671Z","end":"2025-09-11T15:29:14.950863Z","steps":["trace[1853430108] 'process raft request'  (duration: 272.640778ms)"],"step_count":1}
{"level":"warn","ts":"2025-09-11T15:29:15.194097Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"202.560871ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-09-11T15:29:15.205534Z","caller":"traceutil/trace.go:171","msg":"trace[38298966] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:3054; }","duration":"370.069288ms","start":"2025-09-11T15:29:14.835082Z","end":"2025-09-11T15:29:15.205151Z","steps":["trace[38298966] 'agreement among raft nodes before linearized reading'  (duration: 202.495066ms)"],"step_count":1}
{"level":"warn","ts":"2025-09-11T15:29:15.208745Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"210.227715ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 ","response":"range_response_count:1 size:1109"}
{"level":"info","ts":"2025-09-11T15:29:15.208943Z","caller":"traceutil/trace.go:171","msg":"trace[931994469] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:3054; }","duration":"361.236163ms","start":"2025-09-11T15:29:14.847680Z","end":"2025-09-11T15:29:15.208916Z","steps":["trace[931994469] 'agreement among raft nodes before linearized reading'  (duration: 199.00821ms)"],"step_count":1}
{"level":"warn","ts":"2025-09-11T15:29:15.209631Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-09-11T15:29:14.847634Z","time spent":"361.69383ms","remote":"127.0.0.1:57856","response type":"/etcdserverpb.KV/Range","request count":0,"request size":69,"response count":1,"response size":1133,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 "}
{"level":"warn","ts":"2025-09-11T15:29:15.237643Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"440.803562ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/networkpolicies/\" range_end:\"/registry/networkpolicies0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-09-11T15:29:15.237753Z","caller":"traceutil/trace.go:171","msg":"trace[1557279001] range","detail":"{range_begin:/registry/networkpolicies/; range_end:/registry/networkpolicies0; response_count:0; response_revision:3054; }","duration":"440.957184ms","start":"2025-09-11T15:29:14.796761Z","end":"2025-09-11T15:29:15.237718Z","steps":["trace[1557279001] 'agreement among raft nodes before linearized reading'  (duration: 171.721404ms)"],"step_count":1}
{"level":"warn","ts":"2025-09-11T15:29:15.237784Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-09-11T15:29:14.796729Z","time spent":"441.047372ms","remote":"127.0.0.1:57996","response type":"/etcdserverpb.KV/Range","request count":0,"request size":58,"response count":0,"response size":29,"request content":"key:\"/registry/networkpolicies/\" range_end:\"/registry/networkpolicies0\" count_only:true "}
{"level":"warn","ts":"2025-09-11T15:29:15.259888Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-09-11T15:29:14.632638Z","time spent":"322.776949ms","remote":"127.0.0.1:57976","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":673,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:3047 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:600 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >"}
{"level":"warn","ts":"2025-09-11T15:30:34.007504Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"111.308924ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/validatingadmissionpolicybindings/\" range_end:\"/registry/validatingadmissionpolicybindings0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-09-11T15:30:34.014528Z","caller":"traceutil/trace.go:171","msg":"trace[1961164956] range","detail":"{range_begin:/registry/validatingadmissionpolicybindings/; range_end:/registry/validatingadmissionpolicybindings0; response_count:0; response_revision:3117; }","duration":"131.261099ms","start":"2025-09-11T15:30:33.880587Z","end":"2025-09-11T15:30:34.011848Z","steps":["trace[1961164956] 'get authentication metadata'  (duration: 111.130076ms)"],"step_count":1}
{"level":"warn","ts":"2025-09-11T15:31:51.385312Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.060526033s","expected-duration":"100ms","prefix":"","request":"header:<ID:8128039891666971593 username:\"kube-apiserver-etcd-client\" auth_revision:1 > lease_grant:<ttl:15-second id:70cc99379f03abc8>","response":"size:41"}
{"level":"warn","ts":"2025-09-11T15:31:51.509500Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"411.907113ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/poddisruptionbudgets/\" range_end:\"/registry/poddisruptionbudgets0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"warn","ts":"2025-09-11T15:31:51.418587Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-09-11T15:31:50.010922Z","time spent":"1.398544043s","remote":"127.0.0.1:57708","response type":"/etcdserverpb.Lease/LeaseGrant","request count":-1,"request size":-1,"response count":-1,"response size":-1,"request content":""}
{"level":"info","ts":"2025-09-11T15:31:51.492154Z","caller":"traceutil/trace.go:171","msg":"trace[2055906840] linearizableReadLoop","detail":"{readStateIndex:3740; appliedIndex:3739; }","duration":"323.633944ms","start":"2025-09-11T15:31:51.084014Z","end":"2025-09-11T15:31:51.407648Z","steps":["trace[2055906840] 'read index received'  (duration: 120.773Âµs)","trace[2055906840] 'applied index is now lower than readState.Index'  (duration: 323.511266ms)"],"step_count":2}
{"level":"info","ts":"2025-09-11T15:31:51.587611Z","caller":"traceutil/trace.go:171","msg":"trace[481508371] range","detail":"{range_begin:/registry/poddisruptionbudgets/; range_end:/registry/poddisruptionbudgets0; response_count:0; response_revision:3178; }","duration":"427.840401ms","start":"2025-09-11T15:31:51.082349Z","end":"2025-09-11T15:31:51.510190Z","steps":["trace[481508371] 'agreement among raft nodes before linearized reading'  (duration: 332.308233ms)","trace[481508371] 'count revisions from in-memory index tree'  (duration: 81.158381ms)"],"step_count":2}
{"level":"warn","ts":"2025-09-11T15:31:51.608609Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-09-11T15:31:51.082277Z","time spent":"526.291849ms","remote":"127.0.0.1:58058","response type":"/etcdserverpb.KV/Range","request count":0,"request size":68,"response count":0,"response size":29,"request content":"key:\"/registry/poddisruptionbudgets/\" range_end:\"/registry/poddisruptionbudgets0\" count_only:true "}
{"level":"warn","ts":"2025-09-11T15:31:51.685454Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"572.574661ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-09-11T15:31:51.685640Z","caller":"traceutil/trace.go:171","msg":"trace[1410879242] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:3179; }","duration":"573.885616ms","start":"2025-09-11T15:31:51.111633Z","end":"2025-09-11T15:31:51.685519Z","steps":["trace[1410879242] 'agreement among raft nodes before linearized reading'  (duration: 492.263714ms)","trace[1410879242] 'range keys from in-memory index tree'  (duration: 81.026524ms)"],"step_count":2}
{"level":"warn","ts":"2025-09-11T15:31:51.686050Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-09-11T15:31:51.111608Z","time spent":"574.415669ms","remote":"127.0.0.1:57692","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2025-09-11T15:31:51.686207Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"479.731445ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-09-11T15:31:51.686258Z","caller":"traceutil/trace.go:171","msg":"trace[1819607199] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:3179; }","duration":"479.783973ms","start":"2025-09-11T15:31:51.206458Z","end":"2025-09-11T15:31:51.686241Z","steps":["trace[1819607199] 'agreement among raft nodes before linearized reading'  (duration: 397.488652ms)","trace[1819607199] 'range keys from in-memory index tree'  (duration: 82.180699ms)"],"step_count":2}
{"level":"warn","ts":"2025-09-11T15:31:52.408794Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"269.506936ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-09-11T15:31:53.493342Z","caller":"traceutil/trace.go:171","msg":"trace[1621989461] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:3180; }","duration":"573.239685ms","start":"2025-09-11T15:31:52.139040Z","end":"2025-09-11T15:31:52.712280Z","steps":["trace[1621989461] 'range keys from in-memory index tree'  (duration: 269.454679ms)"],"step_count":1}
{"level":"info","ts":"2025-09-11T15:31:59.683359Z","caller":"traceutil/trace.go:171","msg":"trace[1181201213] transaction","detail":"{read_only:false; response_revision:3183; number_of_response:1; }","duration":"281.317063ms","start":"2025-09-11T15:31:59.402020Z","end":"2025-09-11T15:31:59.683337Z","steps":["trace[1181201213] 'process raft request'  (duration: 281.163686ms)"],"step_count":1}
{"level":"info","ts":"2025-09-11T15:33:43.059935Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3029}
{"level":"info","ts":"2025-09-11T15:33:43.138141Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":3029,"took":"73.567462ms","hash":1807475771,"current-db-size-bytes":2531328,"current-db-size":"2.5 MB","current-db-size-in-use-bytes":1839104,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-09-11T15:33:43.138290Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1807475771,"revision":3029,"compact-revision":2664}
{"level":"info","ts":"2025-09-11T15:38:43.039236Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3272}
{"level":"info","ts":"2025-09-11T15:38:43.106485Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":3272,"took":"57.934084ms","hash":4141593056,"current-db-size-bytes":2596864,"current-db-size":"2.6 MB","current-db-size-in-use-bytes":1994752,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2025-09-11T15:38:43.106740Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":4141593056,"revision":3272,"compact-revision":3029}


==> kernel <==
 15:42:14 up  2:29,  0 users,  load average: 0.46, 0.97, 1.12
Linux minikube 6.6.87.2-microsoft-standard-WSL2 #1 SMP PREEMPT_DYNAMIC Thu Jun  5 18:30:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [a7e9d75f2ce6] <==
I0911 07:13:17.462986       1 controller.go:667] quota admission added evaluator for: namespaces
I0911 07:13:17.470221       1 aggregator.go:171] initial CRD sync complete...
I0911 07:13:17.470299       1 autoregister_controller.go:144] Starting autoregister controller
I0911 07:13:17.470310       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0911 07:13:17.470366       1 cache.go:39] Caches are synced for autoregister controller
I0911 07:13:17.485301       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I0911 07:13:17.544089       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
I0911 07:13:17.548648       1 default_servicecidr_controller.go:214] Setting default ServiceCIDR condition Ready to True
I0911 07:13:17.566945       1 default_servicecidr_controller.go:136] Shutting down kubernetes-service-cidr-controller
I0911 07:13:17.567382       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0911 07:13:18.328038       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0911 07:13:18.346733       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0911 07:13:18.346796       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0911 07:13:19.384994       1 controller.go:667] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0911 07:13:19.586547       1 controller.go:667] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0911 07:13:19.808474       1 alloc.go:328] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0911 07:13:19.827269       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0911 07:13:19.828911       1 controller.go:667] quota admission added evaluator for: endpoints
I0911 07:13:19.842268       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0911 07:13:20.444119       1 controller.go:667] quota admission added evaluator for: serviceaccounts
I0911 07:13:20.778943       1 controller.go:667] quota admission added evaluator for: deployments.apps
I0911 07:13:20.879105       1 alloc.go:328] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0911 07:13:20.945496       1 controller.go:667] quota admission added evaluator for: daemonsets.apps
I0911 07:13:25.348733       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0911 07:13:25.359158       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0911 07:13:26.047968       1 controller.go:667] quota admission added evaluator for: controllerrevisions.apps
I0911 07:13:26.252274       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I0911 07:13:56.394562       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0911 07:13:56.394598       1 alloc.go:328] "allocated clusterIPs" service="default/springboot-single-endpoint-eks" clusterIPs={"IPv4":"10.97.18.9"}
E0911 14:50:25.440010       1 authentication.go:75] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0911 14:50:25.920849       1 authentication.go:75] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0911 14:51:04.197218       1 finisher.go:175] "Unhandled Error" err="FinishRequest: post-timeout activity - time-elapsed: 60.529294ms, panicked: false, err: context canceled, panic-reason: <nil>" logger="UnhandledError"
E0911 14:51:04.588186       1 writers.go:123] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E0911 14:51:04.589722       1 writers.go:123] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E0911 14:51:04.779967       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E0911 14:51:04.884924       1 writers.go:136] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E0911 14:51:04.886625       1 finisher.go:175] "Unhandled Error" err="FinishRequest: post-timeout activity - time-elapsed: 290.554562ms, panicked: false, err: context canceled, panic-reason: <nil>" logger="UnhandledError"
E0911 14:51:04.894103       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E0911 14:51:04.964628       1 writers.go:136] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E0911 14:51:04.968296       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="1.240147542s" method="PUT" path="/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath" result=null
E0911 14:51:04.977375       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="812.24901ms" method="PUT" path="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube" result=null
E0911 14:56:24.464147       1 authentication.go:75] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0911 14:56:24.570399       1 authentication.go:75] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
I0911 14:56:24.690446       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0911 14:58:46.959699       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
E0911 15:01:23.992280       1 authentication.go:75] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0911 15:01:24.098940       1 authentication.go:75] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
I0911 15:01:33.700578       1 alloc.go:328] "allocated clusterIPs" service="default/springboot-single-endpoint-eks" clusterIPs={"IPv4":"10.104.18.252"}
I0911 15:01:33.704776       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
E0911 15:01:33.719651       1 authentication.go:75] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
I0911 15:08:46.866056       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0911 15:17:28.426429       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0911 15:18:46.761875       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0911 15:22:24.291421       1 alloc.go:328] "allocated clusterIPs" service="default/springboot-app-ek8" clusterIPs={"IPv4":"10.98.72.16"}
I0911 15:22:24.298651       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0911 15:28:46.653498       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0911 15:36:32.558559       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0911 15:37:17.077330       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0911 15:37:17.078485       1 alloc.go:328] "allocated clusterIPs" service="default/springboot-app-ek8" clusterIPs={"IPv4":"10.103.109.7"}
I0911 15:38:46.555028       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12


==> kube-controller-manager [7891befb3ea2] <==
I0911 07:13:24.994772       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0911 07:13:24.997128       1 shared_informer.go:357] "Caches are synced" controller="cronjob"
I0911 07:13:25.000794       1 shared_informer.go:357] "Caches are synced" controller="expand"
I0911 07:13:25.055317       1 shared_informer.go:357] "Caches are synced" controller="validatingadmissionpolicy-status"
I0911 07:13:25.055650       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrapproving"
I0911 07:13:25.056024       1 shared_informer.go:357] "Caches are synced" controller="crt configmap"
I0911 07:13:25.057207       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice_mirroring"
I0911 07:13:25.057398       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I0911 07:13:25.058274       1 shared_informer.go:357] "Caches are synced" controller="service-cidr-controller"
I0911 07:13:25.058783       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I0911 07:13:25.058851       1 shared_informer.go:357] "Caches are synced" controller="bootstrap_signer"
I0911 07:13:25.058970       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I0911 07:13:25.059139       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I0911 07:13:25.059241       1 shared_informer.go:357] "Caches are synced" controller="node"
I0911 07:13:25.059342       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0911 07:13:25.059452       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0911 07:13:25.059463       1 shared_informer.go:350] "Waiting for caches to sync" controller="cidrallocator"
I0911 07:13:25.059471       1 shared_informer.go:357] "Caches are synced" controller="cidrallocator"
I0911 07:13:25.060337       1 shared_informer.go:357] "Caches are synced" controller="service account"
I0911 07:13:25.063844       1 shared_informer.go:350] "Waiting for caches to sync" controller="garbage collector"
I0911 07:13:25.068218       1 shared_informer.go:357] "Caches are synced" controller="TTL after finished"
I0911 07:13:25.074705       1 shared_informer.go:357] "Caches are synced" controller="PV protection"
I0911 07:13:25.076284       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I0911 07:13:25.076625       1 shared_informer.go:357] "Caches are synced" controller="ClusterRoleAggregator"
I0911 07:13:25.080720       1 shared_informer.go:357] "Caches are synced" controller="namespace"
I0911 07:13:25.081938       1 shared_informer.go:357] "Caches are synced" controller="TTL"
I0911 07:13:25.151516       1 shared_informer.go:357] "Caches are synced" controller="daemon sets"
I0911 07:13:25.151552       1 shared_informer.go:357] "Caches are synced" controller="legacy-service-account-token-cleaner"
I0911 07:13:25.153063       1 shared_informer.go:357] "Caches are synced" controller="attach detach"
I0911 07:13:25.155533       1 shared_informer.go:357] "Caches are synced" controller="endpoint"
I0911 07:13:25.158824       1 shared_informer.go:357] "Caches are synced" controller="job"
I0911 07:13:25.171095       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice"
I0911 07:13:25.173436       1 shared_informer.go:357] "Caches are synced" controller="HPA"
I0911 07:13:25.173696       1 shared_informer.go:357] "Caches are synced" controller="taint"
I0911 07:13:25.174073       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0911 07:13:25.173697       1 shared_informer.go:357] "Caches are synced" controller="ReplicaSet"
I0911 07:13:25.174279       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0911 07:13:25.174326       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0911 07:13:25.174842       1 shared_informer.go:357] "Caches are synced" controller="PVC protection"
I0911 07:13:25.179028       1 shared_informer.go:357] "Caches are synced" controller="ReplicationController"
I0911 07:13:25.181062       1 shared_informer.go:357] "Caches are synced" controller="ephemeral"
I0911 07:13:25.184261       1 shared_informer.go:357] "Caches are synced" controller="persistent volume"
I0911 07:13:25.224184       1 shared_informer.go:357] "Caches are synced" controller="GC"
I0911 07:13:25.245921       1 shared_informer.go:357] "Caches are synced" controller="taint-eviction-controller"
I0911 07:13:25.275858       1 shared_informer.go:357] "Caches are synced" controller="stateful set"
I0911 07:13:25.361952       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0911 07:13:25.372168       1 shared_informer.go:357] "Caches are synced" controller="disruption"
I0911 07:13:25.380322       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0911 07:13:25.383105       1 shared_informer.go:357] "Caches are synced" controller="deployment"
I0911 07:13:25.851932       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0911 07:13:25.851996       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0911 07:13:25.852014       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0911 07:13:25.865150       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
E0911 14:50:25.445097       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I0911 14:50:25.922872       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
I0911 14:56:24.577767       1 servicecidrs_controller.go:344] "error updating default ServiceCIDR status" logger="service-cidr-controller" error="Unauthorized"
E0911 15:01:24.139372       1 replica_set.go:562] "Unhandled Error" err="sync \"default/springboot-single-endpoint-eks-656784f5ff\" failed with Unauthorized" logger="UnhandledError"
I0911 15:01:33.724109       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="default/springboot-single-endpoint-eks" err="failed to create EndpointSlice for Service default/springboot-single-endpoint-eks: Unauthorized"
I0911 15:01:33.725234       1 event.go:377] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"springboot-single-endpoint-eks", UID:"69fd9629-a3b8-4613-9dae-ab55a80f4d12", APIVersion:"v1", ResourceVersion:"1269", FieldPath:""}): type: 'Warning' reason: 'FailedToUpdateEndpointSlices' Error updating Endpoint Slices for Service default/springboot-single-endpoint-eks: failed to create EndpointSlice for Service default/springboot-single-endpoint-eks: Unauthorized
I0911 15:17:15.758782       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="default/springboot-single-endpoint-eks" err="EndpointSlice informer cache is out of date"


==> kube-proxy [2975ecdf4603] <==
I0911 07:13:28.275093       1 server_linux.go:63] "Using iptables proxy"
I0911 07:13:28.632809       1 server.go:715] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0911 07:13:28.632915       1 server.go:245] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0911 07:13:28.685088       1 server.go:254] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0911 07:13:28.685152       1 server_linux.go:145] "Using iptables Proxier"
I0911 07:13:28.692795       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0911 07:13:28.694709       1 server.go:516] "Version info" version="v1.33.1"
I0911 07:13:28.694738       1 server.go:518] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0911 07:13:28.697359       1 config.go:199] "Starting service config controller"
I0911 07:13:28.697769       1 config.go:105] "Starting endpoint slice config controller"
I0911 07:13:28.698948       1 config.go:440] "Starting serviceCIDR config controller"
I0911 07:13:28.699143       1 shared_informer.go:350] "Waiting for caches to sync" controller="endpoint slice config"
I0911 07:13:28.700188       1 shared_informer.go:350] "Waiting for caches to sync" controller="service config"
I0911 07:13:28.700650       1 shared_informer.go:350] "Waiting for caches to sync" controller="serviceCIDR config"
I0911 07:13:28.702644       1 config.go:329] "Starting node config controller"
I0911 07:13:28.702666       1 shared_informer.go:350] "Waiting for caches to sync" controller="node config"
I0911 07:13:28.799867       1 shared_informer.go:357] "Caches are synced" controller="endpoint slice config"
I0911 07:13:28.801107       1 shared_informer.go:357] "Caches are synced" controller="serviceCIDR config"
I0911 07:13:28.803442       1 shared_informer.go:357] "Caches are synced" controller="node config"
I0911 07:13:28.803476       1 shared_informer.go:357] "Caches are synced" controller="service config"


==> kube-scheduler [7790a1c4bb4a] <==
I0911 07:13:13.449347       1 serving.go:386] Generated self-signed cert in-memory
W0911 07:13:17.357861       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0911 07:13:17.357945       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0911 07:13:17.357967       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0911 07:13:17.358075       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0911 07:13:17.555303       1 server.go:171] "Starting Kubernetes Scheduler" version="v1.33.1"
I0911 07:13:17.555379       1 server.go:173] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0911 07:13:17.567062       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I0911 07:13:17.567258       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0911 07:13:17.567288       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0911 07:13:17.567399       1 shared_informer.go:350] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E0911 07:13:17.578813       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E0911 07:13:17.646875       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0911 07:13:17.646875       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0911 07:13:17.646987       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0911 07:13:17.647658       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0911 07:13:17.647364       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0911 07:13:17.648083       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0911 07:13:17.648410       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0911 07:13:17.648411       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0911 07:13:17.648618       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0911 07:13:17.648618       1 reflector.go:200] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0911 07:13:17.648780       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0911 07:13:17.648829       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0911 07:13:17.648849       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0911 07:13:17.649489       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0911 07:13:17.649897       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0911 07:13:18.511561       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0911 07:13:18.537334       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0911 07:13:18.612964       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0911 07:13:18.779981       1 reflector.go:200] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0911 07:13:18.801977       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E0911 07:13:18.805066       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0911 07:13:18.837711       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0911 07:13:18.941694       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0911 07:13:19.009490       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0911 07:13:19.010969       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0911 07:13:19.026985       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0911 07:13:19.032795       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0911 07:13:19.067534       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0911 07:13:19.074650       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
I0911 07:13:20.968254       1 shared_informer.go:357] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kubelet <==
Sep 11 15:39:00 minikube kubelet[2496]: E0911 15:39:00.935515    2496 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="springboot-single-endpoint-eks:release2"
Sep 11 15:39:00 minikube kubelet[2496]: E0911 15:39:00.935600    2496 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="springboot-single-endpoint-eks:release2"
Sep 11 15:39:00 minikube kubelet[2496]: E0911 15:39:00.936035    2496 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:springboot-app-ek8,Image:springboot-single-endpoint-eks:release2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7bcb8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod springboot-app-ek8-5d8654f8c9-k2fc2_default(95c8a65e-5302-4c96-9056-881f2c9ce615): ErrImagePull: Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Sep 11 15:39:00 minikube kubelet[2496]: E0911 15:39:00.937631    2496 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app-ek8\" with ErrImagePull: \"Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-ek8-5d8654f8c9-k2fc2" podUID="95c8a65e-5302-4c96-9056-881f2c9ce615"
Sep 11 15:39:04 minikube kubelet[2496]: E0911 15:39:04.481358    2496 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="springboot-single-endpoint-eks:release2"
Sep 11 15:39:04 minikube kubelet[2496]: E0911 15:39:04.481521    2496 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="springboot-single-endpoint-eks:release2"
Sep 11 15:39:04 minikube kubelet[2496]: E0911 15:39:04.481765    2496 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:springboot-app-ek8,Image:springboot-single-endpoint-eks:release2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r7r9x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod springboot-app-ek8-5d8654f8c9-dgt9d_default(43c44c5a-0767-41a9-bff4-3afec6171a80): ErrImagePull: Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Sep 11 15:39:04 minikube kubelet[2496]: E0911 15:39:04.483240    2496 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app-ek8\" with ErrImagePull: \"Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-ek8-5d8654f8c9-dgt9d" podUID="43c44c5a-0767-41a9-bff4-3afec6171a80"
Sep 11 15:39:11 minikube kubelet[2496]: E0911 15:39:11.466427    2496 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app-ek8\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-single-endpoint-eks:release2\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-ek8-5d8654f8c9-xmsxv" podUID="4b44b898-d09b-4cbf-a1e2-1a73798917b1"
Sep 11 15:39:13 minikube kubelet[2496]: E0911 15:39:13.466880    2496 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app-ek8\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-single-endpoint-eks:release2\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-ek8-5d8654f8c9-k2fc2" podUID="95c8a65e-5302-4c96-9056-881f2c9ce615"
Sep 11 15:39:17 minikube kubelet[2496]: E0911 15:39:17.469701    2496 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app-ek8\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-single-endpoint-eks:release2\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-ek8-5d8654f8c9-dgt9d" podUID="43c44c5a-0767-41a9-bff4-3afec6171a80"
Sep 11 15:39:25 minikube kubelet[2496]: E0911 15:39:25.467975    2496 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app-ek8\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-single-endpoint-eks:release2\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-ek8-5d8654f8c9-xmsxv" podUID="4b44b898-d09b-4cbf-a1e2-1a73798917b1"
Sep 11 15:39:26 minikube kubelet[2496]: E0911 15:39:26.468005    2496 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app-ek8\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-single-endpoint-eks:release2\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-ek8-5d8654f8c9-k2fc2" podUID="95c8a65e-5302-4c96-9056-881f2c9ce615"
Sep 11 15:39:32 minikube kubelet[2496]: E0911 15:39:32.463160    2496 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app-ek8\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-single-endpoint-eks:release2\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-ek8-5d8654f8c9-dgt9d" podUID="43c44c5a-0767-41a9-bff4-3afec6171a80"
Sep 11 15:39:39 minikube kubelet[2496]: E0911 15:39:39.462008    2496 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app-ek8\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-single-endpoint-eks:release2\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-ek8-5d8654f8c9-k2fc2" podUID="95c8a65e-5302-4c96-9056-881f2c9ce615"
Sep 11 15:39:40 minikube kubelet[2496]: E0911 15:39:40.462560    2496 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app-ek8\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-single-endpoint-eks:release2\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-ek8-5d8654f8c9-xmsxv" podUID="4b44b898-d09b-4cbf-a1e2-1a73798917b1"
Sep 11 15:39:44 minikube kubelet[2496]: E0911 15:39:44.464469    2496 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app-ek8\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-single-endpoint-eks:release2\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-ek8-5d8654f8c9-dgt9d" podUID="43c44c5a-0767-41a9-bff4-3afec6171a80"
Sep 11 15:39:51 minikube kubelet[2496]: E0911 15:39:51.462414    2496 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app-ek8\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-single-endpoint-eks:release2\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-ek8-5d8654f8c9-k2fc2" podUID="95c8a65e-5302-4c96-9056-881f2c9ce615"
Sep 11 15:39:52 minikube kubelet[2496]: E0911 15:39:52.462878    2496 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app-ek8\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-single-endpoint-eks:release2\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-ek8-5d8654f8c9-xmsxv" podUID="4b44b898-d09b-4cbf-a1e2-1a73798917b1"
Sep 11 15:39:57 minikube kubelet[2496]: E0911 15:39:57.461789    2496 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app-ek8\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-single-endpoint-eks:release2\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-ek8-5d8654f8c9-dgt9d" podUID="43c44c5a-0767-41a9-bff4-3afec6171a80"
Sep 11 15:40:02 minikube kubelet[2496]: E0911 15:40:02.459560    2496 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app-ek8\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-single-endpoint-eks:release2\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-ek8-5d8654f8c9-k2fc2" podUID="95c8a65e-5302-4c96-9056-881f2c9ce615"
Sep 11 15:40:08 minikube kubelet[2496]: E0911 15:40:08.460760    2496 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app-ek8\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-single-endpoint-eks:release2\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-ek8-5d8654f8c9-dgt9d" podUID="43c44c5a-0767-41a9-bff4-3afec6171a80"
Sep 11 15:40:09 minikube kubelet[2496]: E0911 15:40:09.097330    2496 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="springboot-single-endpoint-eks:release2"
Sep 11 15:40:09 minikube kubelet[2496]: E0911 15:40:09.097407    2496 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="springboot-single-endpoint-eks:release2"
Sep 11 15:40:09 minikube kubelet[2496]: E0911 15:40:09.097542    2496 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:springboot-app-ek8,Image:springboot-single-endpoint-eks:release2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q46qj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod springboot-app-ek8-5d8654f8c9-xmsxv_default(4b44b898-d09b-4cbf-a1e2-1a73798917b1): ErrImagePull: Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Sep 11 15:40:09 minikube kubelet[2496]: E0911 15:40:09.099903    2496 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app-ek8\" with ErrImagePull: \"Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-ek8-5d8654f8c9-xmsxv" podUID="4b44b898-d09b-4cbf-a1e2-1a73798917b1"
Sep 11 15:40:14 minikube kubelet[2496]: E0911 15:40:14.458980    2496 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app-ek8\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-single-endpoint-eks:release2\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-ek8-5d8654f8c9-k2fc2" podUID="95c8a65e-5302-4c96-9056-881f2c9ce615"
Sep 11 15:40:20 minikube kubelet[2496]: E0911 15:40:20.458230    2496 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app-ek8\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-single-endpoint-eks:release2\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-ek8-5d8654f8c9-xmsxv" podUID="4b44b898-d09b-4cbf-a1e2-1a73798917b1"
Sep 11 15:40:20 minikube kubelet[2496]: E0911 15:40:20.459467    2496 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app-ek8\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-single-endpoint-eks:release2\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-ek8-5d8654f8c9-dgt9d" podUID="43c44c5a-0767-41a9-bff4-3afec6171a80"
Sep 11 15:40:29 minikube kubelet[2496]: E0911 15:40:29.066617    2496 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="springboot-single-endpoint-eks:release2"
Sep 11 15:40:29 minikube kubelet[2496]: E0911 15:40:29.066910    2496 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="springboot-single-endpoint-eks:release2"
Sep 11 15:40:29 minikube kubelet[2496]: E0911 15:40:29.067202    2496 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:springboot-app-ek8,Image:springboot-single-endpoint-eks:release2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7bcb8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod springboot-app-ek8-5d8654f8c9-k2fc2_default(95c8a65e-5302-4c96-9056-881f2c9ce615): ErrImagePull: Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Sep 11 15:40:29 minikube kubelet[2496]: E0911 15:40:29.068761    2496 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app-ek8\" with ErrImagePull: \"Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-ek8-5d8654f8c9-k2fc2" podUID="95c8a65e-5302-4c96-9056-881f2c9ce615"
Sep 11 15:40:33 minikube kubelet[2496]: E0911 15:40:33.455850    2496 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app-ek8\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-single-endpoint-eks:release2\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-ek8-5d8654f8c9-xmsxv" podUID="4b44b898-d09b-4cbf-a1e2-1a73798917b1"
Sep 11 15:40:34 minikube kubelet[2496]: E0911 15:40:34.998567    2496 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="springboot-single-endpoint-eks:release2"
Sep 11 15:40:34 minikube kubelet[2496]: E0911 15:40:34.998678    2496 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="springboot-single-endpoint-eks:release2"
Sep 11 15:40:34 minikube kubelet[2496]: E0911 15:40:34.998827    2496 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:springboot-app-ek8,Image:springboot-single-endpoint-eks:release2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r7r9x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod springboot-app-ek8-5d8654f8c9-dgt9d_default(43c44c5a-0767-41a9-bff4-3afec6171a80): ErrImagePull: Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Sep 11 15:40:35 minikube kubelet[2496]: E0911 15:40:35.000546    2496 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app-ek8\" with ErrImagePull: \"Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-ek8-5d8654f8c9-dgt9d" podUID="43c44c5a-0767-41a9-bff4-3afec6171a80"
Sep 11 15:40:40 minikube kubelet[2496]: E0911 15:40:40.455885    2496 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app-ek8\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-single-endpoint-eks:release2\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-ek8-5d8654f8c9-k2fc2" podUID="95c8a65e-5302-4c96-9056-881f2c9ce615"
Sep 11 15:40:46 minikube kubelet[2496]: E0911 15:40:46.455541    2496 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app-ek8\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-single-endpoint-eks:release2\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-ek8-5d8654f8c9-dgt9d" podUID="43c44c5a-0767-41a9-bff4-3afec6171a80"
Sep 11 15:40:49 minikube kubelet[2496]: E0911 15:40:49.499615    2496 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app-ek8\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-single-endpoint-eks:release2\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-ek8-5d8654f8c9-xmsxv" podUID="4b44b898-d09b-4cbf-a1e2-1a73798917b1"
Sep 11 15:40:53 minikube kubelet[2496]: E0911 15:40:53.454243    2496 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app-ek8\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-single-endpoint-eks:release2\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-ek8-5d8654f8c9-k2fc2" podUID="95c8a65e-5302-4c96-9056-881f2c9ce615"
Sep 11 15:40:59 minikube kubelet[2496]: E0911 15:40:59.455230    2496 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app-ek8\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-single-endpoint-eks:release2\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-ek8-5d8654f8c9-dgt9d" podUID="43c44c5a-0767-41a9-bff4-3afec6171a80"
Sep 11 15:41:02 minikube kubelet[2496]: E0911 15:41:02.450705    2496 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app-ek8\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-single-endpoint-eks:release2\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-ek8-5d8654f8c9-xmsxv" podUID="4b44b898-d09b-4cbf-a1e2-1a73798917b1"
Sep 11 15:41:07 minikube kubelet[2496]: E0911 15:41:07.451713    2496 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app-ek8\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-single-endpoint-eks:release2\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-ek8-5d8654f8c9-k2fc2" podUID="95c8a65e-5302-4c96-9056-881f2c9ce615"
Sep 11 15:41:10 minikube kubelet[2496]: E0911 15:41:10.451055    2496 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app-ek8\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-single-endpoint-eks:release2\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-ek8-5d8654f8c9-dgt9d" podUID="43c44c5a-0767-41a9-bff4-3afec6171a80"
Sep 11 15:41:15 minikube kubelet[2496]: E0911 15:41:15.450986    2496 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app-ek8\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-single-endpoint-eks:release2\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-ek8-5d8654f8c9-xmsxv" podUID="4b44b898-d09b-4cbf-a1e2-1a73798917b1"
Sep 11 15:41:19 minikube kubelet[2496]: E0911 15:41:19.450762    2496 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app-ek8\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-single-endpoint-eks:release2\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-ek8-5d8654f8c9-k2fc2" podUID="95c8a65e-5302-4c96-9056-881f2c9ce615"
Sep 11 15:41:23 minikube kubelet[2496]: E0911 15:41:23.450205    2496 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app-ek8\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-single-endpoint-eks:release2\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-ek8-5d8654f8c9-dgt9d" podUID="43c44c5a-0767-41a9-bff4-3afec6171a80"
Sep 11 15:41:30 minikube kubelet[2496]: E0911 15:41:30.453780    2496 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app-ek8\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-single-endpoint-eks:release2\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-ek8-5d8654f8c9-xmsxv" podUID="4b44b898-d09b-4cbf-a1e2-1a73798917b1"
Sep 11 15:41:31 minikube kubelet[2496]: E0911 15:41:31.455477    2496 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app-ek8\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-single-endpoint-eks:release2\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-ek8-5d8654f8c9-k2fc2" podUID="95c8a65e-5302-4c96-9056-881f2c9ce615"
Sep 11 15:41:38 minikube kubelet[2496]: E0911 15:41:38.446416    2496 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app-ek8\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-single-endpoint-eks:release2\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-ek8-5d8654f8c9-dgt9d" podUID="43c44c5a-0767-41a9-bff4-3afec6171a80"
Sep 11 15:41:45 minikube kubelet[2496]: E0911 15:41:45.446465    2496 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app-ek8\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-single-endpoint-eks:release2\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-ek8-5d8654f8c9-xmsxv" podUID="4b44b898-d09b-4cbf-a1e2-1a73798917b1"
Sep 11 15:41:46 minikube kubelet[2496]: E0911 15:41:46.445588    2496 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app-ek8\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-single-endpoint-eks:release2\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-ek8-5d8654f8c9-k2fc2" podUID="95c8a65e-5302-4c96-9056-881f2c9ce615"
Sep 11 15:41:49 minikube kubelet[2496]: E0911 15:41:49.448975    2496 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app-ek8\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-single-endpoint-eks:release2\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-ek8-5d8654f8c9-dgt9d" podUID="43c44c5a-0767-41a9-bff4-3afec6171a80"
Sep 11 15:41:59 minikube kubelet[2496]: E0911 15:41:59.446519    2496 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app-ek8\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-single-endpoint-eks:release2\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-ek8-5d8654f8c9-k2fc2" podUID="95c8a65e-5302-4c96-9056-881f2c9ce615"
Sep 11 15:41:59 minikube kubelet[2496]: E0911 15:41:59.447431    2496 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app-ek8\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-single-endpoint-eks:release2\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-ek8-5d8654f8c9-xmsxv" podUID="4b44b898-d09b-4cbf-a1e2-1a73798917b1"
Sep 11 15:42:04 minikube kubelet[2496]: E0911 15:42:04.443514    2496 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app-ek8\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-single-endpoint-eks:release2\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-ek8-5d8654f8c9-dgt9d" podUID="43c44c5a-0767-41a9-bff4-3afec6171a80"
Sep 11 15:42:13 minikube kubelet[2496]: E0911 15:42:13.447204    2496 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app-ek8\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-single-endpoint-eks:release2\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-ek8-5d8654f8c9-k2fc2" podUID="95c8a65e-5302-4c96-9056-881f2c9ce615"
Sep 11 15:42:14 minikube kubelet[2496]: E0911 15:42:14.444169    2496 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"springboot-app-ek8\" with ImagePullBackOff: \"Back-off pulling image \\\"springboot-single-endpoint-eks:release2\\\": ErrImagePull: Error response from daemon: pull access denied for springboot-single-endpoint-eks, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/springboot-app-ek8-5d8654f8c9-xmsxv" podUID="4b44b898-d09b-4cbf-a1e2-1a73798917b1"


==> storage-provisioner [644c6a8bc3e4] <==
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1.3()
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x5c
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0xc00044cca0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:155 +0x5f
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc00044cca0, 0x18b3d60, 0xc000571d70, 0xc000360001, 0xc00008e600)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:156 +0x9b
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc00044cca0, 0x3b9aca00, 0x0, 0x1, 0xc00008e600)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x98
k8s.io/apimachinery/pkg/util/wait.Until(0xc00044cca0, 0x3b9aca00, 0xc00008e600)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x4d
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x3d6

goroutine 96 [sync.Cond.Wait, 1 minutes]:
sync.runtime_notifyListWait(0xc00058e350, 0x3)
	/usr/local/go/src/runtime/sema.go:513 +0xf8
sync.(*Cond).Wait(0xc00058e340)
	/usr/local/go/src/sync/cond.go:56 +0x99
k8s.io/client-go/util/workqueue.(*Type).Get(0xc0005102a0, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/util/workqueue/queue.go:145 +0x89
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).processNextClaimWorkItem(0xc0003bec80, 0x18e5530, 0xc00032a280, 0x203000)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:935 +0x3e
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).runClaimWorker(...)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:924
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1.2()
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:880 +0x5c
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0xc00044ccc0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:155 +0x5f
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc00044ccc0, 0x18b3d60, 0xc0001ddbf0, 0x1, 0xc00008e600)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:156 +0x9b
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc00044ccc0, 0x3b9aca00, 0x0, 0x1, 0xc00008e600)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x98
k8s.io/apimachinery/pkg/util/wait.Until(0xc00044ccc0, 0x3b9aca00, 0xc00008e600)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x4d
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:880 +0x4af

goroutine 129 [sync.Cond.Wait, 1 minutes]:
sync.runtime_notifyListWait(0xc00058e390, 0x3)
	/usr/local/go/src/runtime/sema.go:513 +0xf8
sync.(*Cond).Wait(0xc00058e380)
	/usr/local/go/src/sync/cond.go:56 +0x99
k8s.io/client-go/util/workqueue.(*Type).Get(0xc000510420, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/util/workqueue/queue.go:145 +0x89
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).processNextVolumeWorkItem(0xc0003bec80, 0x18e5530, 0xc00032a280, 0x203000)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:990 +0x3e
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).runVolumeWorker(...)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:929
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1.3()
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x5c
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0xc00044cce0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:155 +0x5f
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc00044cce0, 0x18b3d60, 0xc000441b00, 0x1, 0xc00008e600)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:156 +0x9b
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc00044cce0, 0x3b9aca00, 0x0, 0x1, 0xc00008e600)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x98
k8s.io/apimachinery/pkg/util/wait.Until(0xc00044cce0, 0x3b9aca00, 0xc00008e600)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x4d
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x3d6


==> storage-provisioner [f5c06664425d] <==
W0911 15:41:14.659581       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:41:14.672104       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:41:16.680797       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:41:16.691511       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:41:18.733524       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:41:18.747599       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:41:20.752602       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:41:20.760964       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:41:22.765900       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:41:22.772195       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:41:24.776799       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:41:24.783858       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:41:26.791550       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:41:26.832897       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:41:28.841517       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:41:28.890078       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:41:30.896624       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:41:30.952235       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:41:32.952765       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:41:32.963227       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:41:34.969467       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:41:34.981087       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:41:36.987310       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:41:37.031420       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:41:39.044011       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:41:39.067728       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:41:41.074840       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:41:41.088840       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:41:43.096596       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:41:43.137682       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:41:45.145773       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:41:45.157471       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:41:47.171682       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:41:47.179257       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:41:49.189855       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:41:49.210579       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:41:51.216926       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:41:51.229217       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:41:53.233384       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:41:53.238665       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:41:55.245114       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:41:55.265851       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:41:57.269504       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:41:57.278294       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:41:59.283516       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:41:59.291243       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:42:01.301372       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:42:01.323215       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:42:03.325629       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:42:03.333977       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:42:05.338832       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:42:05.346943       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:42:07.350975       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:42:07.358470       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:42:09.362866       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:42:09.370436       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:42:11.376074       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:42:11.383244       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:42:13.389399       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0911 15:42:13.407539       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice

